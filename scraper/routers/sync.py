from typing import Optional, Set
from pathlib import Path
import httpx, asyncio

from fastapi import APIRouter, Depends, Query
from sqlalchemy import select, update, delete
from sqlalchemy.ext.asyncio import AsyncSession
from starlette.responses import JSONResponse

from core.database import get_db, AsyncSessionLocal
from models import Provider
from providers import bunjang
from core.logger import setup_logger
from providers.bunjang import fetch_products, fetch_product_detail, upsert_product
from utils.string import parse_korean_number

logger = setup_logger(__name__)  # í˜„ì¬ íŒŒì¼ëª… ê¸°ì¤€ ì´ë¦„ ì§€ì •

router = APIRouter(prefix="/sync", tags=["Sync"])

keyword_queue: list[str] = []         # ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•Šì€ í‚¤ì›Œë“œ
total_keywords: int = 0               # ì „ì²´ í‚¤ì›Œë“œ ìˆ˜
batch_size: int = 5
processed_count: int = 0             # í˜„ì¬ê¹Œì§€ ì²˜ë¦¬ëœ ìˆ˜
is_running: bool = False             # ì‹¤í–‰ ì¤‘ ì—¬ë¶€
all_product_pids: Set[str] = set()    # pid ì¤‘ë³µ ì œê±°ìš© set
pid_lock = asyncio.Lock()             # ë™ì‹œì„± ì œì–´ë¥¼ ìœ„í•œ lock

@router.post("/test")
async def test(
    data: str = Query("13500000", description="ìˆ«ì")
):
    return {"value": parse_korean_number(data)}

@router.post("/sync_categories")
async def sync_categories(
    code: str = Query(..., description="ê³µê¸‰ì ì½”ë“œ"),
    db: AsyncSession = Depends(get_db)
):
    await bunjang.fetch_categories(code, db)
    return {"message": f"{code} categories synced."}

@router.post("/sync_products")
async def sync_products(
    code: str = Query("BUNJANG", description="ê³µê¸‰ì ì½”ë“œ"),
    # keyword: Optional[str] = Query(None, description="í‚¤ì›Œë“œ"),
    category: Optional[str] = Query("750800", description="ì¹´í…Œê³ ë¦¬"),
    db: AsyncSession = Depends(get_db)
):
    global keyword_queue, total_keywords, processed_count, is_running, all_product_pids

    if is_running:
        return JSONResponse(
            status_code=409,
            content={
                "message": "ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.",
                "progress": f"{processed_count}/{total_keywords}"
            }
        )

    result = await db.execute(select(Provider).where(Provider.code == code))
    provider = result.scalars().first()

    # í‚¤ì›Œë“œ ë¡œë”©
    text = Path("./list.txt").read_text(encoding="utf-8")
    keywords = []
    for line in text.splitlines():
        for item in line.split(","):
            kw = item.strip()
            if kw and kw not in keywords:
                keywords.append(kw)

    keyword_queue = keywords
    total_keywords = len(keyword_queue)
    processed_count = 0
    all_product_pids.clear()

    async def sync_detail_and_save(pid: str):
        async with AsyncSessionLocal() as db:
            detail = await fetch_product_detail(pid)
            if detail:
                await upsert_product(db, detail)

    async def run_keyword_sync():
        global is_running, processed_count, all_product_pids
        is_running = True
        try:
            logger.info("ğŸš€ í‚¤ì›Œë“œ ë™ê¸°í™” ì‹œì‘")

            # 1. pid ìˆ˜ì§‘
            async def worker(worker_id: int):
                global keyword_queue, processed_count
                while True:
                    try:
                        keyword = keyword_queue.pop(0)
                    except IndexError:
                        break
                    try:
                        if not provider or provider.code == "BUNJANG":
                            all_product_pids.update(await fetch_products(keyword=keyword, category=category))
                            processed_count += 1
                    except Exception:
                        logger.exception(f"[{keyword}] âŒ ì‘ì—…ì {worker_id} ì—ëŸ¬ ë°œìƒ")

            tasks = [asyncio.create_task(worker(i)) for i in range(batch_size)]
            await asyncio.gather(*tasks)
            logger.info(f"âœ… pid ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(all_product_pids)}ê°œ")

            # 2. ìƒì„¸ ì •ë³´ upsert
            pids = list(all_product_pids)
            pid_chunks = [pids[i:i+batch_size] for i in range(0, len(pids), batch_size)]
            for chunk in pid_chunks:
                detail_tasks = [
                    asyncio.create_task(sync_detail_and_save(pid))
                    for pid in chunk
                ]
                await asyncio.gather(*detail_tasks)

            logger.info("âœ… ìƒì„¸ upsert ì™„ë£Œ.")

        except Exception:
            logger.exception("âŒ run_keyword_sync ì—ëŸ¬ ë°œìƒ")
        finally:
            is_running = False

    asyncio.create_task(run_keyword_sync())
    logger.info(f"total_keywords: {total_keywords}, keyword_queue: {keyword_queue}")
    return {"message": f"{total_keywords}ê°œ í‚¤ì›Œë“œ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ë° ìƒì„¸ upsert ì‹œì‘"}