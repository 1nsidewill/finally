#!/usr/bin/env python3
"""
Redis ì—°ê²° í’€ ìµœì í™” ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
ë‹¤ì–‘í•œ ì—°ê²° í’€ ì„¤ì •ìœ¼ë¡œ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì—¬ ìµœì ì˜ ì„¤ì •ì„ ì°¾ìŠµë‹ˆë‹¤.
"""

import asyncio
import time
import json
import statistics
from datetime import datetime
from typing import List, Dict, Any, Tuple
import redis.asyncio as redis
from src.config import get_settings
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RedisPoolBenchmark:
    """Redis ì—°ê²° í’€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self):
        self.config = get_settings()
        self.test_queue = "test_pool_optimization"
        self.results: List[Dict[str, Any]] = []
        
    async def create_pool_config(self, max_connections: int, socket_timeout: float, 
                                connect_timeout: float, retry_on_timeout: bool = True) -> redis.ConnectionPool:
        """íŠ¹ì • ì„¤ì •ìœ¼ë¡œ ì—°ê²° í’€ ìƒì„±"""
        if self.config.REDIS_URL:
            pool = redis.ConnectionPool.from_url(
                self.config.REDIS_URL,
                max_connections=max_connections,
                socket_connect_timeout=connect_timeout,
                socket_timeout=socket_timeout,
                retry_on_timeout=retry_on_timeout,
                encoding='utf-8',
                decode_responses=True
            )
        else:
            pool = redis.ConnectionPool(
                host=self.config.REDIS_HOST,
                port=self.config.REDIS_PORT,
                db=self.config.REDIS_DB,
                password=self.config.REDIS_PASSWORD,
                max_connections=max_connections,
                socket_connect_timeout=connect_timeout,
                socket_timeout=socket_timeout,
                retry_on_timeout=retry_on_timeout,
                encoding='utf-8',
                decode_responses=True
            )
        
        return pool
    
    async def benchmark_pool_config(self, max_connections: int, socket_timeout: float,
                                  connect_timeout: float, test_jobs: int = 1000,
                                  batch_size: int = 30) -> Dict[str, Any]:
        """íŠ¹ì • ì—°ê²° í’€ ì„¤ì •ìœ¼ë¡œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        config_name = f"max_conn_{max_connections}_sock_timeout_{socket_timeout}_conn_timeout_{connect_timeout}"
        logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œì‘: {config_name}")
        
        pool = await self.create_pool_config(max_connections, socket_timeout, connect_timeout)
        redis_client = redis.Redis(connection_pool=pool)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = [
            {
                "id": i,
                "action": "sync",
                "product_uid": f"test-product-{i}",
                "data": {"index": i, "timestamp": time.time()}
            }
            for i in range(test_jobs)
        ]
        
        try:
            # í ì´ˆê¸°í™”
            await redis_client.delete(self.test_queue)
            
            # === Push ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ===
            push_times = []
            push_start = time.time()
            
            # ë°°ì¹˜ë¡œ ì‘ì—… í‘¸ì‹œ
            for i in range(0, test_jobs, batch_size):
                batch = test_data[i:i + batch_size]
                batch_json = [json.dumps(job, ensure_ascii=False) for job in batch]
                
                batch_start = time.time()
                await redis_client.lpush(self.test_queue, *batch_json)
                batch_end = time.time()
                
                push_times.append(batch_end - batch_start)
            
            push_total_time = time.time() - push_start
            
            # === Pop ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ===
            pop_times = []
            pop_start = time.time()
            popped_jobs = 0
            
            while popped_jobs < test_jobs:
                # ë°°ì¹˜ë¡œ ì‘ì—… íŒ
                pipe = redis_client.pipeline()
                current_batch_size = min(batch_size, test_jobs - popped_jobs)
                
                for _ in range(current_batch_size):
                    pipe.rpop(self.test_queue)
                
                batch_start = time.time()
                results = await pipe.execute()
                batch_end = time.time()
                
                # ê²°ê³¼ ì²˜ë¦¬
                valid_results = [r for r in results if r is not None]
                popped_jobs += len(valid_results)
                
                if valid_results:
                    pop_times.append(batch_end - batch_start)
                
                if not valid_results:  # íê°€ ë¹„ì—ˆìŒ
                    break
            
            pop_total_time = time.time() - pop_start
            
            # === ì—°ê²° í’€ í†µê³„ ===
            pool_stats = {
                "created_connections": pool.connection_kwargs.get("max_connections", 0),
                "pool_size": max_connections
            }
            
            # === ê²°ê³¼ ê³„ì‚° ===
            result = {
                "config": {
                    "max_connections": max_connections,
                    "socket_timeout": socket_timeout,
                    "connect_timeout": connect_timeout,
                    "batch_size": batch_size
                },
                "test_params": {
                    "total_jobs": test_jobs,
                    "batches_count": len(push_times)
                },
                "push_performance": {
                    "total_time": push_total_time,
                    "jobs_per_second": test_jobs / push_total_time,
                    "avg_batch_time": statistics.mean(push_times) if push_times else 0,
                    "min_batch_time": min(push_times) if push_times else 0,
                    "max_batch_time": max(push_times) if push_times else 0
                },
                "pop_performance": {
                    "total_time": pop_total_time,
                    "jobs_per_second": popped_jobs / pop_total_time if pop_total_time > 0 else 0,
                    "avg_batch_time": statistics.mean(pop_times) if pop_times else 0,
                    "min_batch_time": min(pop_times) if pop_times else 0,
                    "max_batch_time": max(pop_times) if pop_times else 0,
                    "jobs_processed": popped_jobs
                },
                "overall_performance": {
                    "total_time": push_total_time + pop_total_time,
                    "overall_jobs_per_second": test_jobs / (push_total_time + pop_total_time),
                    "push_pop_ratio": push_total_time / pop_total_time if pop_total_time > 0 else 0
                },
                "pool_stats": pool_stats,
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"âœ… {config_name} ì™„ë£Œ - {result['overall_performance']['overall_jobs_per_second']:.1f} jobs/sec")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ {config_name} ì‹¤íŒ¨: {e}")
            return {
                "config": {"max_connections": max_connections, "socket_timeout": socket_timeout, "connect_timeout": connect_timeout},
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
            
        finally:
            # ì •ë¦¬
            try:
                await redis_client.delete(self.test_queue)
                await pool.disconnect()
            except:
                pass
    
    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """í¬ê´„ì ì¸ Redis ì—°ê²° í’€ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        logger.info("ğŸš€ Redis ì—°ê²° í’€ ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        
        # í…ŒìŠ¤íŠ¸í•  ì„¤ì •ë“¤
        test_configurations = [
            # (max_connections, socket_timeout, connect_timeout)
            (10, 5.0, 5.0),    # ì‘ì€ í’€
            (20, 5.0, 5.0),    # í˜„ì¬ ì„¤ì •
            (30, 5.0, 5.0),    # ì¤‘ê°„ í’€
            (50, 5.0, 5.0),    # ê¶Œì¥ ì„¤ì •
            (75, 5.0, 5.0),    # í° í’€
            (100, 5.0, 5.0),   # ìµœëŒ€ í’€
            
            # íƒ€ì„ì•„ì›ƒ ìµœì í™” í…ŒìŠ¤íŠ¸ (50 ì—°ê²°ë¡œ ê³ ì •)
            (50, 2.0, 2.0),    # ì§§ì€ íƒ€ì„ì•„ì›ƒ
            (50, 3.0, 3.0),    # ì¤‘ê°„ íƒ€ì„ì•„ì›ƒ 
            (50, 10.0, 10.0),  # ê¸´ íƒ€ì„ì•„ì›ƒ
        ]
        
        results = []
        
        for max_conn, sock_timeout, conn_timeout in test_configurations:
            try:
                result = await self.benchmark_pool_config(
                    max_connections=max_conn,
                    socket_timeout=sock_timeout,
                    connect_timeout=conn_timeout,
                    test_jobs=500,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¤„ì„
                    batch_size=30   # í˜„ì¬ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì‚¬ìš©
                )
                results.append(result)
                
                # í…ŒìŠ¤íŠ¸ ê°„ ì ì‹œ ëŒ€ê¸°
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"ì„¤ì • {max_conn}/{sock_timeout}/{conn_timeout} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ ë¶„ì„
        analysis = self.analyze_results(results)
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"redis_pool_benchmark_{timestamp}.json"
        
        final_result = {
            "benchmark_info": {
                "timestamp": datetime.now().isoformat(),
                "test_count": len(results),
                "jobs_per_test": 500,
                "batch_size": 30
            },
            "results": results,
            "analysis": analysis
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ! ê²°ê³¼ ì €ì¥: {output_file}")
        
        return final_result
    
    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„"""
        
        valid_results = [r for r in results if "error" not in r]
        
        if not valid_results:
            return {"error": "ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ì„±ëŠ¥ ì§€í‘œë³„ ë¶„ì„
        performance_by_config = []
        
        for result in valid_results:
            config = result["config"]
            perf = result["overall_performance"]
            
            performance_by_config.append({
                "max_connections": config["max_connections"],
                "socket_timeout": config["socket_timeout"],
                "connect_timeout": config["connect_timeout"],
                "jobs_per_second": perf["overall_jobs_per_second"],
                "total_time": perf["total_time"],
                "push_jobs_per_second": result["push_performance"]["jobs_per_second"],
                "pop_jobs_per_second": result["pop_performance"]["jobs_per_second"]
            })
        
        # ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
        best_overall = max(performance_by_config, key=lambda x: x["jobs_per_second"])
        best_push = max(performance_by_config, key=lambda x: x["push_jobs_per_second"])
        best_pop = max(performance_by_config, key=lambda x: x["pop_jobs_per_second"])
        
        # í˜„ì¬ ì„¤ì • (20 ì—°ê²°) ì„±ëŠ¥
        current_config = next((p for p in performance_by_config 
                              if p["max_connections"] == 20 and p["socket_timeout"] == 5.0), None)
        
        analysis = {
            "best_overall_performance": best_overall,
            "best_push_performance": best_push,
            "best_pop_performance": best_pop,
            "current_config_performance": current_config,
            "performance_summary": performance_by_config,
            "recommendations": self.generate_recommendations(performance_by_config, current_config)
        }
        
        return analysis
    
    def generate_recommendations(self, performance_data: List[Dict], current_config: Dict) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        # ì—°ê²° ìˆ˜ë³„ ì„±ëŠ¥ ë¶„ì„
        conn_performance = {}
        for perf in performance_data:
            conn_count = perf["max_connections"]
            if conn_count not in conn_performance:
                conn_performance[conn_count] = []
            conn_performance[conn_count].append(perf["jobs_per_second"])
        
        # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
        avg_performance = {
            conn: statistics.mean(perfs) 
            for conn, perfs in conn_performance.items()
        }
        
        # ìµœì  ì—°ê²° ìˆ˜ ì°¾ê¸°
        optimal_connections = max(avg_performance.items(), key=lambda x: x[1])
        
        recommendations = {
            "optimal_max_connections": optimal_connections[0],
            "expected_performance_improvement": f"{optimal_connections[1] / current_config['jobs_per_second']:.2f}x" if current_config else "N/A",
            "connection_analysis": avg_performance,
            "summary": f"ì—°ê²° ìˆ˜ {optimal_connections[0]}ë¡œ ì„¤ì • ì‹œ ìµœê³  ì„±ëŠ¥ ({optimal_connections[1]:.1f} jobs/sec)"
        }
        
        return recommendations

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    benchmark = RedisPoolBenchmark()
    
    try:
        # Redis ì—°ê²° í™•ì¸
        config = get_settings()
        test_redis = redis.from_url(config.REDIS_URL) if config.REDIS_URL else redis.Redis(host=config.REDIS_HOST, port=config.REDIS_PORT)
        
        if not await test_redis.ping():
            raise Exception("Redis ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        await test_redis.close()
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        results = await benchmark.run_comprehensive_benchmark()
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ† REDIS ì—°ê²° í’€ ìµœì í™” ê²°ê³¼")
        print("="*80)
        
        if "analysis" in results:
            analysis = results["analysis"]
            
            print(f"\nğŸ“Š í˜„ì¬ ì„¤ì • (ì—°ê²° ìˆ˜ 20):")
            if analysis["current_config_performance"]:
                current = analysis["current_config_performance"]
                print(f"   ì²˜ë¦¬ëŸ‰: {current['jobs_per_second']:.1f} jobs/sec")
                print(f"   ì´ ì‹œê°„: {current['total_time']:.2f}s")
            
            print(f"\nğŸ¯ ìµœì  ì„¤ì •:")
            best = analysis["best_overall_performance"]
            print(f"   ì—°ê²° ìˆ˜: {best['max_connections']}")
            print(f"   ì†Œì¼“ íƒ€ì„ì•„ì›ƒ: {best['socket_timeout']}s")
            print(f"   ì—°ê²° íƒ€ì„ì•„ì›ƒ: {best['connect_timeout']}s")
            print(f"   ì²˜ë¦¬ëŸ‰: {best['jobs_per_second']:.1f} jobs/sec")
            
            print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            recs = analysis["recommendations"]
            print(f"   {recs['summary']}")
            print(f"   ì„±ëŠ¥ í–¥ìƒ: {recs['expected_performance_improvement']}")
        
        print(f"\nğŸ“ ìì„¸í•œ ê²°ê³¼ëŠ” íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”")
        
    except Exception as e:
        logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main()) 