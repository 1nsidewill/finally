#!/usr/bin/env python3
"""
ğŸš€ Enhanced Bulk Synchronizer with Advanced Progress Tracking
í–¥ìƒëœ ì§„í–‰ë¥  ì¶”ì ì´ í¬í•¨ëœ ëŒ€ìš©ëŸ‰ ë™ê¸°í™” ì‹œìŠ¤í…œ
"""

import asyncio
import time
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path

from ..database.postgresql import PostgreSQLManager
from ..database.qdrant import QdrantManager
from ..monitoring.progress_tracker import ProgressTracker, track_progress
from ..config import get_settings

logger = logging.getLogger(__name__)

class EnhancedBulkSynchronizer:
    """ğŸš€ í–¥ìƒëœ ëŒ€ìš©ëŸ‰ ë™ê¸°í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, batch_size: int = 50, max_retries: int = 3):
        self.batch_size = batch_size
        self.max_retries = max_retries
        self.config = get_settings()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì €ë“¤
        self.pg_manager = PostgreSQLManager()
        self.qdrant_manager = QdrantManager()
        
        logger.info(f"ğŸš€ Enhanced Bulk Synchronizer ì´ˆê¸°í™” (ë°°ì¹˜ í¬ê¸°: {batch_size})")
    
    async def sync_all_products(
        self, 
        session_id: Optional[str] = None,
        use_optimized_batch: bool = True,
        parallel_batches: int = 3
    ) -> Dict[str, Any]:
        """ëª¨ë“  ì œí’ˆì„ Qdrantì™€ ë™ê¸°í™” (í–¥ìƒëœ ì§„í–‰ë¥  ì¶”ì  í¬í•¨)"""
        
        if not session_id:
            session_id = f"bulk_sync_{int(time.time())}"
        
        logger.info(f"ğŸ¯ ëŒ€ìš©ëŸ‰ ë™ê¸°í™” ì‹œì‘: {session_id}")
        
        try:
            # 1. ì´ ì²˜ë¦¬í•  ì œí’ˆ ìˆ˜ ì¡°íšŒ
            total_products = await self._get_total_products()
            if total_products == 0:
                return {
                    "status": "success",
                    "message": "ì²˜ë¦¬í•  ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤",
                    "processed": 0
                }
            
            total_batches = (total_products + self.batch_size - 1) // self.batch_size
            logger.info(f"ğŸ“Š ì²˜ë¦¬ ê³„íš: {total_products}ê°œ ì œí’ˆ, {total_batches}ê°œ ë°°ì¹˜")
            
            # 2. ì§„í–‰ë¥  ì¶”ì  ì‹œì‘
            async with track_progress(session_id, total_products, total_batches) as tracker:
                
                # ì§„í–‰ë¥  ì½œë°± ì„¤ì •
                def on_progress_update(session):
                    logger.info(
                        f"ğŸ“ˆ ì „ì²´ ì§„í–‰ë¥ : {session.completion_percentage:.1f}% "
                        f"({session.processed_items}/{session.total_items})"
                    )
                    if session.estimated_time_remaining:
                        logger.info(f"â±ï¸ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {session.estimated_time_remaining}")
                
                def on_batch_complete(batch):
                    logger.info(
                        f"âœ… ë°°ì¹˜ {batch.batch_id} ì™„ë£Œ: "
                        f"{batch.successful_items}/{batch.total_items} ì„±ê³µ "
                        f"({batch.success_rate:.1f}%), ì†ë„: {batch.processing_rate:.2f}/s"
                    )
                
                tracker.add_progress_callback(on_progress_update)
                tracker.add_batch_callback(on_batch_complete)
                
                # 3. ë°°ì¹˜ë³„ ì²˜ë¦¬ ì‹¤í–‰
                total_successful = 0
                total_failed = 0
                
                for batch_id in range(total_batches):
                    offset = batch_id * self.batch_size
                    
                    # ë°°ì¹˜ ì‹œì‘
                    current_batch_size = min(self.batch_size, total_products - offset)
                    batch = tracker.start_batch(batch_id, current_batch_size)
                    
                    try:
                        # ë°°ì¹˜ ì²˜ë¦¬
                        batch_result = await self._process_batch_enhanced(
                            batch_id, offset, current_batch_size, tracker,
                            use_optimized_batch, parallel_batches
                        )
                        
                        total_successful += batch_result["successful"]
                        total_failed += batch_result["failed"]
                        
                        # ë°°ì¹˜ ì™„ë£Œ
                        tracker.complete_batch(batch_id)
                        
                        # ë°°ì¹˜ ê°„ ì ì‹œ ëŒ€ê¸° (ì‹œìŠ¤í…œ ë¶€í•˜ ì¡°ì ˆ)
                        if batch_id < total_batches - 1:
                            await asyncio.sleep(0.5)
                    
                    except Exception as e:
                        logger.error(f"ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        tracker.update_batch_progress(
                            batch_id, current_batch_size, 0, current_batch_size,
                            [{"batch_id": batch_id, "error": str(e)}]
                        )
                        tracker.complete_batch(batch_id)
                        total_failed += current_batch_size
                
                # 4. ìµœì¢… ê²°ê³¼
                final_result = {
                    "status": "success",
                    "session_id": session_id,
                    "total_products": total_products,
                    "successful": total_successful,
                    "failed": total_failed,
                    "success_rate": (total_successful / total_products * 100) if total_products > 0 else 0,
                    "processing_time_seconds": tracker.session.duration_seconds if hasattr(tracker.session, 'duration_seconds') else 0,
                    "average_rate": tracker.session.average_processing_rate,
                    "peak_memory_mb": tracker.session.peak_memory_usage_mb
                }
                
                logger.info(f"ğŸ‰ ëŒ€ìš©ëŸ‰ ë™ê¸°í™” ì™„ë£Œ: {final_result}")
                return final_result
        
        except Exception as e:
            logger.error(f"ëŒ€ìš©ëŸ‰ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "session_id": session_id,
                "error": str(e),
                "processed": 0
            }
    
    async def _process_batch_enhanced(
        self, 
        batch_id: int, 
        offset: int, 
        batch_size: int, 
        tracker: ProgressTracker,
        use_optimized_batch: bool = True,
        parallel_batches: int = 3
    ) -> Dict[str, Any]:
        """í–¥ìƒëœ ë°°ì¹˜ ì²˜ë¦¬"""
        
        logger.debug(f"ğŸ”„ ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì‹œì‘ (ì˜¤í”„ì…‹: {offset}, í¬ê¸°: {batch_size})")
        
        successful = 0
        failed = 0
        error_details = []
        
        try:
            # 1. ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ
            products = await self._get_products_batch(offset, batch_size)
            if not products:
                logger.warning(f"ë°°ì¹˜ {batch_id}: ì¡°íšŒëœ ì œí’ˆì´ ì—†ìŒ")
                return {"successful": 0, "failed": 0}
            
            # 2. ê°œë³„ ì œí’ˆ ì²˜ë¦¬ ë˜ëŠ” ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬
            if use_optimized_batch and len(products) > 10:
                # ìµœì í™”ëœ ë°°ì¹˜ ì—…ë¡œë“œ ì‚¬ìš©
                result = await self._process_optimized_batch(
                    products, batch_id, tracker, parallel_batches
                )
                successful = result["successful"]
                failed = result["failed"]
                error_details = result.get("errors", [])
            else:
                # ê°œë³„ ì²˜ë¦¬ (ì†Œê·œëª¨ ë°°ì¹˜ ë˜ëŠ” ë””ë²„ê¹…ìš©)
                for i, product in enumerate(products):
                    try:
                        await self._process_single_product(product)
                        successful += 1
                        
                        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        tracker.update_batch_progress(
                            batch_id, i + 1, successful, failed
                        )
                        
                    except Exception as e:
                        failed += 1
                        error_detail = {
                            "product_uid": product.get("uid"),
                            "error": str(e),
                            "timestamp": datetime.now().isoformat()
                        }
                        error_details.append(error_detail)
                        
                        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        tracker.update_batch_progress(
                            batch_id, i + 1, successful, failed, [error_detail]
                        )
                        
                        logger.warning(f"ì œí’ˆ {product.get('uid')} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            logger.info(f"âœ… ë°°ì¹˜ {batch_id} ì™„ë£Œ: ì„±ê³µ {successful}, ì‹¤íŒ¨ {failed}")
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            failed = batch_size
            error_details.append({
                "batch_id": batch_id,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
        
        return {
            "successful": successful,
            "failed": failed,
            "errors": error_details
        }
    
    async def _process_optimized_batch(
        self, 
        products: List[Dict[str, Any]], 
        batch_id: int, 
        tracker: ProgressTracker,
        parallel_batches: int = 3
    ) -> Dict[str, Any]:
        """ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬"""
        
        logger.debug(f"ğŸš€ ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬: {len(products)}ê°œ ì œí’ˆ")
        
        try:
            # 1. ì„ë² ë”© ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
            embeddings_data = []
            successful = 0
            failed = 0
            errors = []
            
            # ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ ì„¸ë§ˆí¬ì–´
            embedding_semaphore = asyncio.Semaphore(parallel_batches)
            
            async def generate_embedding_for_product(product, index):
                async with embedding_semaphore:
                    try:
                        # í…ìŠ¤íŠ¸ ìƒì„±
                        text = f"{product.get('title', '')} {product.get('brand', '')} {product.get('content', '')}"
                        
                        # ì„ë² ë”© ìƒì„±
                        embedding = await self.qdrant_manager.generate_embedding(text)
                        
                        return {
                            "product": product,
                            "embedding": embedding,
                            "index": index,
                            "success": True
                        }
                    except Exception as e:
                        return {
                            "product": product,
                            "error": str(e),
                            "index": index,
                            "success": False
                        }
            
            # ëª¨ë“  ì œí’ˆì— ëŒ€í•´ ë³‘ë ¬ë¡œ ì„ë² ë”© ìƒì„±
            embedding_tasks = [
                generate_embedding_for_product(product, i)
                for i, product in enumerate(products)
            ]
            
            embedding_results = await asyncio.gather(*embedding_tasks)
            
            # ê²°ê³¼ ë¶„ë¥˜
            successful_embeddings = []
            for result in embedding_results:
                if result["success"]:
                    embeddings_data.append({
                        "product": result["product"],
                        "embedding": result["embedding"]
                    })
                    successful += 1
                else:
                    failed += 1
                    errors.append({
                        "product_uid": result["product"].get("uid"),
                        "error": f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {result['error']}",
                        "timestamp": datetime.now().isoformat()
                    })
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                tracker.update_batch_progress(
                    batch_id, result["index"] + 1, successful, failed
                )
            
            # 2. Qdrantì— ë°°ì¹˜ ì—…ë¡œë“œ (ì„±ê³µí•œ ì„ë² ë”©ë§Œ)
            if embeddings_data:
                await self._batch_upload_to_qdrant(embeddings_data)
                
                # 3. PostgreSQL ìƒíƒœ ì—…ë°ì´íŠ¸
                successful_uids = [item["product"]["uid"] for item in embeddings_data]
                await self._batch_update_postgresql(successful_uids)
            
            return {
                "successful": successful,
                "failed": failed,
                "errors": errors
            }
            
        except Exception as e:
            logger.error(f"ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "successful": 0,
                "failed": len(products),
                "errors": [{
                    "batch_error": str(e),
                    "timestamp": datetime.now().isoformat()
                }]
            }
    
    async def _batch_upload_to_qdrant(self, embeddings_data: List[Dict[str, Any]]):
        """Qdrantì— ë°°ì¹˜ ì—…ë¡œë“œ"""
        from qdrant_client.http.models import PointStruct
        from ..database.qdrant import generate_product_vector_id
        
        points = []
        for item in embeddings_data:
            product = item["product"]
            embedding = item["embedding"]
            
            # ìƒˆë¡œìš´ UUID ìƒì„± ë¡œì§: uid + provider ê¸°ë°˜
            point_id = generate_product_vector_id(str(product["uid"]), "bunjang")
            
            payload = {
                'uid': product['uid'],
                'title': product.get('title', ''),
                'brand': product.get('brand', ''),
                'content': product.get('content', ''),
                'price': str(product.get('price', '')),
                'status': product.get('status', '')
            }
            
            point = PointStruct(
                id=point_id,
                vector=embedding,
                payload=payload
            )
            points.append(point)
        
        # ìµœì í™”ëœ ë°°ì¹˜ ì—…ë¡œë“œ ì‚¬ìš©
        await self.qdrant_manager.upsert_points_batch_optimized(
            points, batch_size=100, wait=False, parallel_batches=3
        )
        
        logger.debug(f"âœ… Qdrant ë°°ì¹˜ ì—…ë¡œë“œ ì™„ë£Œ: {len(points)}ê°œ í¬ì¸íŠ¸")
    
    async def _batch_update_postgresql(self, successful_uids: List[int]):
        """PostgreSQL ë°°ì¹˜ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        if not successful_uids:
            return
        
        # ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì¿¼ë¦¬
        uid_list = ','.join(map(str, successful_uids))
        query = f"""
        UPDATE product 
        SET is_conversion = true 
        WHERE uid IN ({uid_list})
        """
        
        await self.pg_manager.execute_query(query)
        logger.debug(f"âœ… PostgreSQL ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(successful_uids)}ê°œ ì œí’ˆ")
    
    async def _get_total_products(self) -> int:
        """ì´ ì²˜ë¦¬í•  ì œí’ˆ ìˆ˜ ì¡°íšŒ"""
        query = "SELECT COUNT(*) as total FROM product WHERE is_conversion = false"
        result = await self.pg_manager.execute_query(query)
        return result[0]['total'] if result else 0
    
    async def _get_products_batch(self, offset: int, limit: int) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì œí’ˆ ì¡°íšŒ"""
        query = """
        SELECT uid, pid, title, brand, content, price, status
        FROM product 
        WHERE is_conversion = false
        ORDER BY uid
        LIMIT $1 OFFSET $2
        """
        return await self.pg_manager.execute_query(query, limit, offset)
    
    async def _process_single_product(self, product: Dict[str, Any]):
        """ë‹¨ì¼ ì œí’ˆ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)"""
        # ê¸°ì¡´ ë‹¨ì¼ ì œí’ˆ ì²˜ë¦¬ ë¡œì§
        text = f"{product.get('title', '')} {product.get('brand', '')} {product.get('content', '')}"
        embedding = await self.qdrant_manager.generate_embedding(text)
        
        # Qdrant ì €ì¥
        await self.qdrant_manager.upsert_vector_async(
            product["uid"], product["pid"], embedding, product
        )
        
        # PostgreSQL ì—…ë°ì´íŠ¸
        update_query = "UPDATE product SET is_conversion = true WHERE uid = $1"
        await self.pg_manager.execute_query(update_query, product["uid"])

# ì‚¬ìš© ì˜ˆì‹œ
async def run_enhanced_bulk_sync():
    """í–¥ìƒëœ ëŒ€ìš©ëŸ‰ ë™ê¸°í™” ì‹¤í–‰"""
    synchronizer = EnhancedBulkSynchronizer(batch_size=50)
    
    result = await synchronizer.sync_all_products(
        session_id="enhanced_sync_test",
        use_optimized_batch=True,
        parallel_batches=3
    )
    
    print(f"ğŸ‰ ë™ê¸°í™” ì™„ë£Œ: {result}")

if __name__ == "__main__":
    asyncio.run(run_enhanced_bulk_sync()) 