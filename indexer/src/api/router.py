"""
FastAPI ë¼ìš°í„° ì„¤ì •
ê° ì—”ë“œí¬ì¸íŠ¸ì— ëŒ€í•œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í¬í•¨
"""

from fastapi import APIRouter, HTTPException, Request, Response
from fastapi.responses import PlainTextResponse
from typing import List, Dict, Any, Optional
import logging
import time
import json

from ..database.postgresql import PostgreSQLManager
from ..database.qdrant import QdrantManager
from ..database.redis import RedisManager
from ..services.embedding_service import EmbeddingService
from ..services.failure_handler import FailureHandler
from ..workers.reliable_worker import ReliableWorker
from ..monitoring.metrics import MetricsCollector, get_metrics_bytes
from ..config import get_settings
from .models import (
    SyncRequest, SyncResponse, RetryRequest, RetryResponse,
    QueueStatusResponse, FailedOperation, FailuresResponse
)

logger = logging.getLogger(__name__)
router = APIRouter()

# ì„¤ì • ë¡œë“œ
config = get_settings()

# =============================================================================
# ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.get("/health", tags=["health"])
@MetricsCollector.track_redis_job("api", "health_check")
async def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        # ê° ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸
        pg_manager = PostgreSQLManager()
        qdrant_manager = QdrantManager()
        redis_manager = RedisManager()
        
        status = {
            "status": "healthy",
            "timestamp": time.time(),
            "components": {
                "postgresql": "unknown",
                "qdrant": "unknown", 
                "redis": "unknown"
            }
        }
        
        # PostgreSQL ì—°ê²° í™•ì¸
        try:
            async with pg_manager.get_connection() as conn:
                await conn.execute("SELECT 1")
                status["components"]["postgresql"] = "healthy"
        except Exception as e:
            status["components"]["postgresql"] = f"unhealthy: {str(e)}"
            status["status"] = "degraded"
        
        # Qdrant ì—°ê²° í™•ì¸
        try:
            collections = await qdrant_manager.list_collections()
            status["components"]["qdrant"] = "healthy"
        except Exception as e:
            status["components"]["qdrant"] = f"unhealthy: {str(e)}"
            status["status"] = "degraded"
        
        # Redis ì—°ê²° í™•ì¸
        try:
            await redis_manager.ping()
            status["components"]["redis"] = "healthy"
        except Exception as e:
            status["components"]["redis"] = f"unhealthy: {str(e)}"
            status["status"] = "degraded"
        
        return status
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=500, detail=f"Health check failed: {str(e)}")

# =============================================================================
# ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.get("/metrics", tags=["monitoring"])
async def get_prometheus_metrics():
    """Prometheus ë©”íŠ¸ë¦­ ë°˜í™˜"""
    try:
        metrics_data = get_metrics_bytes()
        return Response(
            content=metrics_data,
            media_type="text/plain; version=0.0.4; charset=utf-8"
        )
    except Exception as e:
        logger.error(f"Failed to get metrics: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get metrics: {str(e)}")

@router.get("/metrics/status", tags=["monitoring"])
@MetricsCollector.track_redis_job("api", "metrics_status")
async def get_metrics_status():
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ìƒíƒœ í™•ì¸"""
    try:
        # Redis í í¬ê¸° í™•ì¸
        redis_manager = RedisManager()
        queue_sizes = {}
        
        try:
            # ê¸°ë³¸ íë“¤ í¬ê¸° í™•ì¸
            embedding_queue_size = await redis_manager.get_queue_size("embedding_queue")
            sync_queue_size = await redis_manager.get_queue_size("sync_queue")
            
            queue_sizes = {
                "embedding_queue": embedding_queue_size,
                "sync_queue": sync_queue_size
            }
            
            # ë©”íŠ¸ë¦­ì— í í¬ê¸° ì—…ë°ì´íŠ¸
            await MetricsCollector.update_queue_size("embedding_queue", embedding_queue_size)
            await MetricsCollector.update_queue_size("sync_queue", sync_queue_size)
            
        except Exception as e:
            logger.warning(f"Failed to get queue sizes: {e}")
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        await MetricsCollector.update_system_metrics()
        
        return {
            "status": "collecting",
            "timestamp": time.time(),
            "queue_sizes": queue_sizes,
            "metrics_endpoint": "/metrics"
        }
        
    except Exception as e:
        logger.error(f"Failed to get metrics status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get metrics status: {str(e)}")

# =============================================================================
# ë™ê¸°í™” ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.post("/sync/trigger", tags=["sync"])
@MetricsCollector.track_redis_job("api", "sync_trigger")
async def trigger_sync():
    """ìˆ˜ë™ ë™ê¸°í™” íŠ¸ë¦¬ê±°"""
    try:
        redis_manager = RedisManager()
        
        # ë™ê¸°í™” ì‘ì—…ì„ Redis íì— ì¶”ê°€
        job_data = {
            "type": "full_sync",
            "timestamp": time.time(),
            "triggered_by": "api"
        }
        
        job_id = await redis_manager.enqueue_job("sync_queue", job_data)
        
        return {
            "message": "Sync triggered successfully",
            "job_id": job_id,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Failed to trigger sync: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to trigger sync: {str(e)}")

@router.get("/sync/status", tags=["sync"])
@MetricsCollector.track_redis_job("api", "sync_status")
async def get_sync_status():
    """ë™ê¸°í™” ìƒíƒœ í™•ì¸"""
    try:
        pg_manager = PostgreSQLManager()
        qdrant_manager = QdrantManager()
        
        # PostgreSQLì—ì„œ ì´ ë§¤ë¬¼ ìˆ˜ í™•ì¸
        async with pg_manager.get_connection() as conn:
            pg_count_result = await conn.fetchrow(
                "SELECT COUNT(*) as total FROM products"
            )
            pg_total = pg_count_result['total'] if pg_count_result else 0
        
        # Qdrantì—ì„œ ì´ ë²¡í„° ìˆ˜ í™•ì¸
        try:
            collection_info = await qdrant_manager.get_collection_info()
            qdrant_total = collection_info.points_count if collection_info else 0
        except Exception as e:
            logger.warning(f"Failed to get Qdrant count: {e}")
            qdrant_total = 0
        
        # ë™ê¸°í™” ìƒíƒœ ê³„ì‚°
        sync_percentage = (qdrant_total / pg_total * 100) if pg_total > 0 else 0
        
        return {
            "postgresql_count": pg_total,
            "qdrant_count": qdrant_total,
            "sync_percentage": round(sync_percentage, 2),
            "needs_sync": pg_total != qdrant_total,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Failed to get sync status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get sync status: {str(e)}")

# =============================================================================
# ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸ (ì¶”í›„ êµ¬í˜„)
# =============================================================================

@router.post("/search", tags=["search"])
@MetricsCollector.track_redis_job("api", "vector_search")
async def vector_search(query: Dict[str, Any]):
    """ë²¡í„° ê²€ìƒ‰ (ì¶”í›„ êµ¬í˜„)"""
    try:
        # í˜„ì¬ëŠ” í”Œë ˆì´ìŠ¤í™€ë”
        return {
            "message": "Vector search endpoint - coming soon",
            "query": query,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Vector search failed: {e}")
        raise HTTPException(status_code=500, detail=f"Vector search failed: {str(e)}")

# =============================================================================
# ë””ë²„ê¹… ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.get("/debug/info", tags=["debug"])
async def get_debug_info():
    """ë””ë²„ê¹… ì •ë³´ ë°˜í™˜"""
    try:
        return {
            "config": {
                "redis_max_connections": config.REDIS_MAX_CONNECTIONS,
                "redis_batch_size": config.REDIS_BATCH_SIZE,
                "embedding_model": config.OPENAI_EMBEDDING_MODEL,
                "qdrant_collection": config.QDRANT_COLLECTION_NAME
            },
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Failed to get debug info: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get debug info: {str(e)}")

# =============================================================================
# Redis Queue ìš´ì˜ ê´€ë¦¬ API
# =============================================================================

@router.post("/sync", tags=["operations"], response_model=SyncResponse)
@MetricsCollector.track_redis_job("api", "manual_sync")
async def manual_sync(request: SyncRequest):
    """íŠ¹ì • ë§¤ë¬¼ì˜ ìˆ˜ë™ ì¬ì²˜ë¦¬"""
    try:
        redis_manager = RedisManager()
        
        # ì‘ì—… ë°ì´í„° êµ¬ì„±
        job_data = {
            "type": "sync",
            "product_uid": request.product_uid,
            "force": request.force,
            "priority": request.priority,
            "timestamp": time.time(),
            "triggered_by": "manual_api"
        }
        
        # Redis íì— ì‘ì—… ì¶”ê°€
        job_id = await redis_manager.enqueue_job("indexer_jobs", job_data)
        
        logger.info(f"Manual sync queued for product {request.product_uid}, job_id: {job_id}")
        
        return SyncResponse(
            message=f"Manual sync queued for product {request.product_uid}",
            job_id=job_id,
            product_uid=request.product_uid,
            timestamp=time.time()
        )
        
    except Exception as e:
        logger.error(f"Failed to queue manual sync: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to queue manual sync: {str(e)}")

@router.post("/retry", tags=["operations"], response_model=RetryResponse)
@MetricsCollector.track_redis_job("api", "retry_failed")
async def retry_failed_operations(request: RetryRequest):
    """ì‹¤íŒ¨í•œ ì‘ì—…ë“¤ì„ ì¬ì‹œë„"""
    try:
        failure_handler = FailureHandler()
        reliable_worker = ReliableWorker()
        
        retried_count = 0
        failed_retry_count = 0
        job_ids = []
        
        # ì¬ì‹œë„í•  ì‘ì—…ë“¤ ì¡°íšŒ
        if request.operation_ids:
            # íŠ¹ì • ì‘ì—… IDë“¤ ì¬ì‹œë„
            for op_id in request.operation_ids:
                try:
                    job_id = await reliable_worker.retry_failed_operation(op_id)
                    if job_id:
                        job_ids.append(job_id)
                        retried_count += 1
                    else:
                        failed_retry_count += 1
                except Exception as e:
                    logger.error(f"Failed to retry operation {op_id}: {e}")
                    failed_retry_count += 1
        else:
            # ì¬ì‹œë„ ê°€ëŠ¥í•œ ëª¨ë“  ì‘ì—… ì¡°íšŒ
            retryable_ops = await failure_handler.get_retryable_operations(
                limit=request.max_operations
            )
            
            for op in retryable_ops:
                try:
                    job_id = await reliable_worker.retry_failed_operation(op.id)
                    if job_id:
                        job_ids.append(job_id)
                        retried_count += 1
                    else:
                        failed_retry_count += 1
                except Exception as e:
                    logger.error(f"Failed to retry operation {op.id}: {e}")
                    failed_retry_count += 1
        
        logger.info(f"Retry completed: {retried_count} success, {failed_retry_count} failed")
        
        return RetryResponse(
            message=f"Retry completed: {retried_count} operations retried, {failed_retry_count} failed",
            retried_count=retried_count,
            failed_retry_count=failed_retry_count,
            job_ids=job_ids,
            timestamp=time.time()
        )
        
    except Exception as e:
        logger.error(f"Failed to retry operations: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to retry operations: {str(e)}")

@router.get("/status", tags=["monitoring"], response_model=QueueStatusResponse)
@MetricsCollector.track_redis_job("api", "queue_status")
async def get_queue_status():
    """Redis í ìƒíƒœ ë° ì›Œì»¤ ì§„í–‰ í˜„í™© ì¡°íšŒ"""
    try:
        redis_manager = RedisManager()
        failure_handler = FailureHandler()
        
        # Redis í ìƒíƒœ
        queue_size = await redis_manager.get_queue_size("indexer_jobs")
        
        # ì‹¤íŒ¨ í†µê³„
        failure_stats = await failure_handler.get_failure_stats()
        
        # ì›Œì»¤ ìƒíƒœ (ê°„ë‹¨í•œ ì¶”ì •)
        worker_status = {
            "estimated_active_workers": min(queue_size, config.REDIS_MAX_CONNECTIONS),
            "queue_throughput_estimate": "49.8 jobs/sec",  # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ê¸°ë°˜
            "estimated_completion_time": f"{queue_size / 49.8:.1f} seconds" if queue_size > 0 else "0 seconds"
        }
        
        # í ìƒì„¸ ì •ë³´
        queue_details = {
            "indexer_jobs": {
                "size": queue_size,
                "estimated_processing_time": worker_status["estimated_completion_time"]
            }
        }
        
        # ì‹¤íŒ¨ ì‘ì—… ìˆ˜ ê³„ì‚°
        total_failed = sum(stats.get('total_failures', 0) for stats in failure_stats.values())
        
        return QueueStatusResponse(
            total_pending=queue_size,
            total_processing=0,  # Redis Queueì—ì„œëŠ” ì •í™•í•œ ì²˜ë¦¬ ì¤‘ ì‘ì—… ìˆ˜ë¥¼ ì•Œê¸° ì–´ë ¤ì›€
            total_failed=total_failed,
            queue_details=queue_details,
            worker_status=worker_status,
            timestamp=time.time()
        )
        
    except Exception as e:
        logger.error(f"Failed to get queue status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get queue status: {str(e)}")

@router.get("/failures", tags=["monitoring"], response_model=FailuresResponse)
@MetricsCollector.track_redis_job("api", "get_failures")
async def get_failed_operations(
    page: int = 1,
    page_size: int = 50,
    operation_type: Optional[str] = None,
    product_uid: Optional[str] = None
):
    """ì‹¤íŒ¨í•œ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
    try:
        failure_handler = FailureHandler()
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ê³„ì‚°
        offset = (page - 1) * page_size
        
        # ì‹¤íŒ¨ ì‘ì—… ì¡°íšŒ (ì„ì‹œë¡œ ê°„ë‹¨í•œ êµ¬í˜„)
        # ì‹¤ì œë¡œëŠ” failure_handlerì—ì„œ í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›í•˜ëŠ” ë©”ì„œë“œ í•„ìš”
        try:
            # PostgreSQLì—ì„œ ì§ì ‘ ì¡°íšŒ
            pg_manager = PostgreSQLManager()
            
            # WHERE ì¡°ê±´ êµ¬ì„± (resolved_atì´ NULLì´ë©´ ì•„ì§ í•´ê²°ë˜ì§€ ì•Šì€ ì‹¤íŒ¨ ì‘ì—…)
            where_conditions = ["resolved_at IS NULL"]
            params = []
            
            if operation_type:
                where_conditions.append(f"operation_type = ${len(params) + 1}")
                params.append(operation_type)
            
            if product_uid:
                where_conditions.append(f"product_uid = ${len(params) + 1}")
                params.append(product_uid)
            
            where_clause = " AND ".join(where_conditions)
            
            # ì´ ê°œìˆ˜ ì¡°íšŒ
            count_query = f"SELECT COUNT(*) as total FROM failed_operations WHERE {where_clause}"
            
            # ë°ì´í„° ì¡°íšŒ
            data_query = f"""
            SELECT id, product_uid, operation_type, error_message, retry_count, max_retries,
                   next_retry_at, created_at, last_attempted_at, error_details
            FROM failed_operations 
            WHERE {where_clause}
            ORDER BY created_at DESC
            LIMIT {page_size} OFFSET {offset}
            """
            
            async with pg_manager.get_connection() as conn:
                # ì´ ê°œìˆ˜
                count_result = await conn.fetchrow(count_query, *params)
                total_count = count_result['total'] if count_result else 0
                
                # ë°ì´í„°
                rows = await conn.fetch(data_query, *params)
                
                failed_operations = []
                for row in rows:
                    # Parse error_details JSON string to dict
                    context = None
                    if row['error_details']:
                        try:
                            if isinstance(row['error_details'], str):
                                context = json.loads(row['error_details'])
                            else:
                                context = row['error_details']  # Already a dict
                        except json.JSONDecodeError:
                            context = {"error": "Failed to parse error_details", "raw": str(row['error_details'])}
                    
                    failed_operations.append(FailedOperation(
                        id=row['id'],
                        product_uid=str(row['product_uid']),  # Convert to string
                        operation_type=row['operation_type'],
                        error_message=row['error_message'],
                        retry_count=row['retry_count'] or 0,
                        max_retries=row['max_retries'] or 3,
                        next_retry_at=row['next_retry_at'],
                        created_at=row['created_at'],
                        updated_at=row['last_attempted_at'] or row['created_at'],  # Use last_attempted_at as updated_at
                        context=context
                    ))
                
                has_more = (page * page_size) < total_count
                
                return FailuresResponse(
                    failed_operations=failed_operations,
                    total_count=total_count,
                    page=page,
                    page_size=page_size,
                    has_more=has_more,
                    timestamp=time.time()
                )
                
        except Exception as e:
            logger.error(f"Failed to query failed operations: {e}")
            # í´ë°±: ë¹ˆ ì‘ë‹µ
            return FailuresResponse(
                failed_operations=[],
                total_count=0,
                page=page,
                page_size=page_size,
                has_more=False,
                timestamp=time.time()
            )
        
    except Exception as e:
        logger.error(f"Failed to get failed operations: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get failed operations: {str(e)}")

# Qdrant Storage Optimization ì—”ë“œí¬ì¸íŠ¸ë“¤ ì¶”ê°€
@router.post("/qdrant/optimize", response_model=Dict[str, Any])
async def optimize_qdrant_collection():
    """ğŸ§¹ Qdrant ì»¬ë ‰ì…˜ ìµœì í™” ì‹¤í–‰"""
    try:
        qdrant_manager = QdrantManager()
        result = await qdrant_manager.optimize_collection()
        
        return {
            "success": True,
            "message": "ì»¬ë ‰ì…˜ ìµœì í™” ì™„ë£Œ",
            "data": result
        }
    except Exception as e:
        logger.error(f"ì»¬ë ‰ì…˜ ìµœì í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì»¬ë ‰ì…˜ ìµœì í™” ì‹¤íŒ¨: {str(e)}")

@router.get("/qdrant/storage-stats", response_model=Dict[str, Any])
async def get_qdrant_storage_stats():
    """ğŸ“Š Qdrant ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ ë° ì„±ëŠ¥ í†µê³„"""
    try:
        qdrant_manager = QdrantManager()
        stats = await qdrant_manager.get_storage_stats()
        
        return {
            "success": True,
            "message": "ìŠ¤í† ë¦¬ì§€ í†µê³„ ì¡°íšŒ ì™„ë£Œ",
            "data": stats
        }
    except Exception as e:
        logger.error(f"ìŠ¤í† ë¦¬ì§€ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ìŠ¤í† ë¦¬ì§€ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.post("/qdrant/batch-upload-optimized", response_model=Dict[str, Any])
async def batch_upload_optimized(
    batch_size: int = 100,
    parallel_batches: int = 3,
    wait: bool = False
):
    """ğŸš€ ìµœì í™”ëœ ë°°ì¹˜ ì—…ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš© - ì‹¤ì œ ë°ì´í„°ëŠ” sync ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©)"""
    try:
        # ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” í…ŒìŠ¤íŠ¸/ë°ëª¨ìš©ì…ë‹ˆë‹¤
        # ì‹¤ì œ ë°ì´í„° ì—…ë¡œë“œëŠ” /sync ì—”ë“œí¬ì¸íŠ¸ë¥¼ í†µí•´ ìˆ˜í–‰ë©ë‹ˆë‹¤
        
        return {
            "success": True,
            "message": "ë°°ì¹˜ ì—…ë¡œë“œ ìµœì í™” ì„¤ì • í™•ì¸ë¨",
            "config": {
                "batch_size": batch_size,
                "parallel_batches": parallel_batches,
                "wait": wait
            },
            "note": "ì‹¤ì œ ë°ì´í„° ì—…ë¡œë“œëŠ” /sync ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
        }
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì—…ë¡œë“œ ì„¤ì • í™•ì¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ì—…ë¡œë“œ ì„¤ì • í™•ì¸ ì‹¤íŒ¨: {str(e)}")

# Progress Tracking ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ë“¤ ì¶”ê°€
@router.post("/sync/enhanced", response_model=Dict[str, Any])
async def sync_data_enhanced(
    batch_size: int = 50,
    use_optimized_batch: bool = True,
    parallel_batches: int = 3,
    session_id: Optional[str] = None
):
    """ğŸš€ í–¥ìƒëœ ì§„í–‰ë¥  ì¶”ì ì´ í¬í•¨ëœ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë™ê¸°í™”"""
    try:
        from ..services.bulk_sync_enhanced import EnhancedBulkSynchronizer
        
        synchronizer = EnhancedBulkSynchronizer(batch_size=batch_size)
        
        result = await synchronizer.sync_all_products(
            session_id=session_id,
            use_optimized_batch=use_optimized_batch,
            parallel_batches=parallel_batches
        )
        
        return {
            "success": True,
            "message": "í–¥ìƒëœ ë™ê¸°í™” ì™„ë£Œ",
            "data": result
        }
    except Exception as e:
        logger.error(f"í–¥ìƒëœ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í–¥ìƒëœ ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")

@router.get("/progress/{session_id}", response_model=Dict[str, Any])
async def get_progress_status(session_id: str):
    """ğŸ“Š íŠ¹ì • ì„¸ì…˜ì˜ ì§„í–‰ë¥  ìƒíƒœ ì¡°íšŒ"""
    try:
        from ..monitoring.progress_tracker import ProgressTracker
        from pathlib import Path
        import json
        
        # ì§„í–‰ë¥  ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        log_dir = Path("./.taskmaster/logs")
        progress_file = log_dir / f"progress_{session_id}.json"
        
        if not progress_file.exists():
            raise HTTPException(status_code=404, detail=f"ì„¸ì…˜ {session_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì§„í–‰ë¥  ë°ì´í„° ë¡œë“œ
        with open(progress_file, 'r', encoding='utf-8') as f:
            progress_data = json.load(f)
        
        return {
            "success": True,
            "message": "ì§„í–‰ë¥  ì¡°íšŒ ì™„ë£Œ",
            "data": progress_data
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì§„í–‰ë¥  ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì§„í–‰ë¥  ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.get("/progress/sessions", response_model=Dict[str, Any])
async def list_progress_sessions():
    """ğŸ“‹ ëª¨ë“  ì§„í–‰ë¥  ì¶”ì  ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ"""
    try:
        from pathlib import Path
        import json
        import glob
        
        log_dir = Path("./.taskmaster/logs")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë“  progress íŒŒì¼ ì°¾ê¸°
        progress_files = glob.glob(str(log_dir / "progress_*.json"))
        
        sessions = []
        for file_path in progress_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                session_info = data["session"]
                session_info["file_path"] = file_path
                sessions.append(session_info)
                
            except Exception as e:
                logger.warning(f"ì„¸ì…˜ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")
        
        # ì‹œì‘ ì‹œê°„ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        sessions.sort(key=lambda x: x.get("start_time", ""), reverse=True)
        
        return {
            "success": True,
            "message": f"{len(sessions)}ê°œ ì„¸ì…˜ ì¡°íšŒ ì™„ë£Œ",
            "data": {
                "sessions": sessions,
                "total_count": len(sessions)
            }
        }
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.get("/progress/{session_id}/logs", response_model=Dict[str, Any])
async def get_session_logs(session_id: str, lines: int = 100):
    """ğŸ“œ íŠ¹ì • ì„¸ì…˜ì˜ ìƒì„¸ ë¡œê·¸ ì¡°íšŒ"""
    try:
        from pathlib import Path
        
        log_dir = Path("./.taskmaster/logs")
        log_file = log_dir / f"detailed_{session_id}.log"
        
        if not log_file.exists():
            raise HTTPException(status_code=404, detail=f"ì„¸ì…˜ {session_id}ì˜ ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë¡œê·¸ íŒŒì¼ ì½ê¸° (ë§ˆì§€ë§‰ Nì¤„)
        with open(log_file, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()
        
        # ìš”ì²­ëœ ì¤„ ìˆ˜ë§Œí¼ ê°€ì ¸ì˜¤ê¸° (ë§ˆì§€ë§‰ë¶€í„°)
        recent_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
        
        return {
            "success": True,
            "message": f"ë¡œê·¸ ì¡°íšŒ ì™„ë£Œ ({len(recent_lines)}ì¤„)",
            "data": {
                "session_id": session_id,
                "total_lines": len(all_lines),
                "returned_lines": len(recent_lines),
                "logs": [line.strip() for line in recent_lines]
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.delete("/progress/{session_id}", response_model=Dict[str, Any])
async def delete_session_logs(session_id: str):
    """ğŸ—‘ï¸ íŠ¹ì • ì„¸ì…˜ì˜ ë¡œê·¸ íŒŒì¼ë“¤ ì‚­ì œ"""
    try:
        from pathlib import Path
        
        log_dir = Path("./.taskmaster/logs")
        progress_file = log_dir / f"progress_{session_id}.json"
        detail_file = log_dir / f"detailed_{session_id}.log"
        
        deleted_files = []
        
        if progress_file.exists():
            progress_file.unlink()
            deleted_files.append(str(progress_file))
        
        if detail_file.exists():
            detail_file.unlink()
            deleted_files.append(str(detail_file))
        
        if not deleted_files:
            raise HTTPException(status_code=404, detail=f"ì„¸ì…˜ {session_id}ì˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "success": True,
            "message": f"ì„¸ì…˜ {session_id} ë¡œê·¸ ì‚­ì œ ì™„ë£Œ",
            "data": {
                "session_id": session_id,
                "deleted_files": deleted_files
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¡œê·¸ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

# ê¸°ì¡´ sync ì—”ë“œí¬ì¸íŠ¸ì— í–¥ìƒëœ ì˜µì…˜ ì¶”ê°€ (ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©)
@router.post("/sync/with-tracking", response_model=SyncResponse)
async def sync_data_with_tracking(
    request: SyncRequest,
    enable_tracking: bool = True,
    session_id: Optional[str] = None
):
    """ğŸ“Š ì§„í–‰ë¥  ì¶”ì  ì˜µì…˜ì´ í¬í•¨ëœ ë°ì´í„° ë™ê¸°í™”"""
    try:
        if enable_tracking:
            # í–¥ìƒëœ ë™ê¸°í™” ì‚¬ìš©
            from ..services.bulk_sync_enhanced import EnhancedBulkSynchronizer
            
            synchronizer = EnhancedBulkSynchronizer(batch_size=50)
            result = await synchronizer.sync_all_products(session_id=session_id)
            
            return SyncResponse(
                status="success",
                message="ì§„í–‰ë¥  ì¶”ì ê³¼ í•¨ê»˜ ë™ê¸°í™” ì™„ë£Œ",
                processed_count=result.get("successful", 0),
                failed_count=result.get("failed", 0),
                details=result
            )
        else:
            # ê¸°ì¡´ ë™ê¸°í™” ë°©ì‹ ì‚¬ìš©
            # ... ê¸°ì¡´ sync ë¡œì§ ...
            return SyncResponse(
                status="success",
                message="ê¸°ë³¸ ë™ê¸°í™” ì™„ë£Œ",
                processed_count=0,
                failed_count=0
            )
    
    except Exception as e:
        logger.error(f"ì¶”ì  ë™ê¸°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì¶”ì  ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")

# =============================================================================
# Worker Daemon ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@router.get("/worker/status", tags=["worker"])
@MetricsCollector.track_redis_job("api", "worker_status")
async def get_worker_status():
    """ì›Œì»¤ ë°ëª¬ ìƒíƒœ í™•ì¸"""
    try:
        # ì›Œì»¤ ë°ëª¬ ì„í¬íŠ¸ (ê¸€ë¡œë²Œ ë³€ìˆ˜)
        try:
            from ..worker_daemon import worker_daemon
            
            # ì›Œì»¤ í†µê³„ ê°€ì ¸ì˜¤ê¸°
            stats = worker_daemon.get_stats()
            
            return {
                "status": "success",
                "message": "ì›Œì»¤ ìƒíƒœ ì¡°íšŒ ì™„ë£Œ",
                "worker": stats,
                "timestamp": time.time()
            }
            
        except ImportError:
            return {
                "status": "error",
                "message": "ì›Œì»¤ ë°ëª¬ì´ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
                "worker": {
                    "daemon_running": False,
                    "error": "Worker daemon not found"
                },
                "timestamp": time.time()
            }
            
    except Exception as e:
        logger.error(f"Failed to get worker status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get worker status: {str(e)}")

@router.post("/worker/start", tags=["worker"])
@MetricsCollector.track_redis_job("api", "worker_start")
async def start_worker():
    """ì›Œì»¤ ë°ëª¬ ì‹œì‘ (í…ŒìŠ¤íŠ¸ ëª©ì )"""
    try:
        # ì°¸ê³ : ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ì›Œì»¤ë¥¼ ì‹¤í–‰í•´ì•¼ í•¨
        return {
            "status": "info",
            "message": "ì›Œì»¤ëŠ” ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤",
            "instructions": [
                "í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ ì‹¤í–‰:",
                "python worker_daemon.py",
                "ë˜ëŠ”",
                "python -m src.worker_daemon"
            ],
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Failed to provide worker start info: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to provide worker start info: {str(e)}")

@router.post("/worker/stop", tags=["worker"])  
@MetricsCollector.track_redis_job("api", "worker_stop")
async def stop_worker():
    """ì›Œì»¤ ë°ëª¬ ì¤‘ì§€ ì‹ í˜¸ (í…ŒìŠ¤íŠ¸ ëª©ì )"""
    try:
        return {
            "status": "info", 
            "message": "ì›Œì»¤ ì¤‘ì§€ëŠ” ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì§ì ‘ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤",
            "instructions": [
                "ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ Ctrl+C ë˜ëŠ” SIGTERM ì‹œê·¸ë„ ì „ì†¡",
                "ë˜ëŠ” í”„ë¡œì„¸ìŠ¤ IDë¥¼ ì°¾ì•„ì„œ kill ëª…ë ¹ ì‚¬ìš©"
            ],
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Failed to provide worker stop info: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to provide worker stop info: {str(e)}")

@router.post("/worker/test-job", tags=["worker", "testing"])
@MetricsCollector.track_redis_job("api", "worker_test_job")
async def submit_test_job(
    job_type: str = "sync",
    product_id: str = "test_12345",
    provider: str = "bunjang"
):
    """ì›Œì»¤ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í…ŒìŠ¤íŠ¸ Job ì œì¶œ"""
    try:
        redis_manager = RedisManager()
        
        # í…ŒìŠ¤íŠ¸ Job ë°ì´í„° êµ¬ì„±
        test_job = {
            "type": job_type,
            "product_id": product_id,
            "provider": provider,
            "product_data": {
                "title": f"í…ŒìŠ¤íŠ¸ ìƒí’ˆ {product_id}",
                "content": "ì›Œì»¤ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í…ŒìŠ¤íŠ¸ ìƒí’ˆì…ë‹ˆë‹¤",
                "price": 10000,
                "created_dt": time.time()
            },
            "timestamp": time.time(),
            "source": "api_test"
        }
        
        # Redis íì— ì¶”ê°€
        job_id = await redis_manager.enqueue_job(config.REDIS_QUEUE_NAME, test_job)
        
        return {
            "status": "success",
            "message": "í…ŒìŠ¤íŠ¸ Job ì œì¶œ ì™„ë£Œ",
            "job_id": job_id,
            "job_data": test_job,
            "queue": config.REDIS_QUEUE_NAME,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Failed to submit test job: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to submit test job: {str(e)}")

# Export alias for compatibility
api_router = router