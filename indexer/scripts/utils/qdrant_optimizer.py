import asyncio
import sys
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

# Add project root to path
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from src.config import get_settings
from src.database.qdrant import QdrantManager
from qdrant_client import models
from qdrant_client.models import Distance, VectorParams, PointStruct

@dataclass
class QdrantOptimizationConfig:
    """Qdrant ìµœì í™” ì„¤ì •"""
    # ì¸ë±ì‹± ì„ê³„ê°’ - ëŒ€ëŸ‰ ë°ì´í„°ë¥¼ ìœ„í•´ ì¦ê°€
    indexing_threshold: int = 50000  # 12,873ê°œë³´ë‹¤ ì¶©ë¶„íˆ í° ê°’
    
    # ë²¡í„° ì••ì¶• ì„¤ì •
    compression_ratio: Optional[float] = None  # None = ì••ì¶• ì•ˆ í•¨
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì •
    batch_size: int = 100  # í•œ ë²ˆì— ì—…ë¡œë“œí•  í¬ì¸íŠ¸ ìˆ˜
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    on_disk_payload: bool = True
    memmap_threshold: Optional[int] = 100000  # ë©”ëª¨ë¦¬ ë§µ ì„ê³„ê°’
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    vacuum_min_vector_number: int = 1000
    default_segment_number: int = 4  # CPU ì½”ì–´ ìˆ˜ì— ë§ì¶¤
    
    # ì„±ëŠ¥ ì„¤ì •
    max_optimization_threads: int = 1  # ë™ì‹œ ìµœì í™” ìŠ¤ë ˆë“œ
    max_indexing_threads: int = 0  # 0 = ìë™ (CPU ì½”ì–´ ìˆ˜ì— ë§ì¶¤)

class OptimizedQdrantManager(QdrantManager):
    """ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ìµœì í™”ëœ Qdrant ê´€ë¦¬ì"""
    
    def __init__(self, optimization_config: Optional[QdrantOptimizationConfig] = None):
        super().__init__()
        self.opt_config = optimization_config or QdrantOptimizationConfig()
        
    async def recreate_optimized_collection(self) -> bool:
        """ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ì»¬ë ‰ì…˜ ì¬ìƒì„±"""
        try:
            client = self.get_sync_client()
            
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ (ì¡´ì¬í•˜ëŠ” ê²½ìš°)
            collections = client.get_collections().collections
            collection_names = [c.name for c in collections]
            
            if self.collection_name in collection_names:
                print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{self.collection_name}' ì‚­ì œ ì¤‘...")
                client.delete_collection(collection_name=self.collection_name)
                print(f"âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
            
            # ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
            print(f"ğŸš€ ìµœì í™”ëœ ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì¤‘...")
            
            # ì••ì¶• ì„¤ì •
            quantization_config = None
            if self.opt_config.compression_ratio:
                quantization_config = models.ScalarQuantization(
                    scalar=models.ScalarQuantizationConfig(
                        type=models.ScalarType.INT8,
                        quantile=0.99,
                        always_ram=False
                    )
                )
            
            # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
            optimizers_config = models.OptimizersConfigDiff(
                deleted_threshold=0.2,
                vacuum_min_vector_number=self.opt_config.vacuum_min_vector_number,
                default_segment_number=self.opt_config.default_segment_number,
                max_segment_size=None,  # ìë™ ì„¤ì •
                memmap_threshold=self.opt_config.memmap_threshold,
                indexing_threshold=self.opt_config.indexing_threshold,
                flush_interval_sec=5,
                max_optimization_threads=self.opt_config.max_optimization_threads,
            )
            
            # HNSW ì„¤ì • (ë²¡í„° ê²€ìƒ‰ ìµœì í™”)
            hnsw_config = models.HnswConfigDiff(
                m=16,  # ì—°ê²° ìˆ˜ (ê¸°ë³¸ê°’)
                ef_construct=200,  # êµ¬ì¶• ì‹œ íƒìƒ‰ í­ (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
                full_scan_threshold=10000,  # ì „ì²´ ìŠ¤ìº” ì„ê³„ê°’
                max_indexing_threads=self.opt_config.max_indexing_threads,
                on_disk=False,  # ì¸ë±ìŠ¤ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€ (ë¹ ë¥¸ ê²€ìƒ‰)
            )
            
            client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=self.vector_size,
                    distance=Distance.COSINE,
                    hnsw_config=hnsw_config,
                    quantization_config=quantization_config,
                    on_disk=False,  # ë²¡í„°ëŠ” ë©”ëª¨ë¦¬ì— (ê²€ìƒ‰ ì„±ëŠ¥)
                ),
                optimizers_config=optimizers_config,
                on_disk_payload=self.opt_config.on_disk_payload,  # í˜ì´ë¡œë“œë§Œ ë””ìŠ¤í¬ì—
            )
            
            print(f"âœ… ìµœì í™”ëœ ì»¬ë ‰ì…˜ '{self.collection_name}' ìƒì„± ì™„ë£Œ")
            print(f"ğŸ”§ ì ìš©ëœ ìµœì í™”:")
            print(f"  â€¢ ì¸ë±ì‹± ì„ê³„ê°’: {self.opt_config.indexing_threshold:,}")
            print(f"  â€¢ ë°°ì¹˜ í¬ê¸°: {self.opt_config.batch_size}")
            print(f"  â€¢ ë””ìŠ¤í¬ í˜ì´ë¡œë“œ: {self.opt_config.on_disk_payload}")
            print(f"  â€¢ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {self.opt_config.default_segment_number}")
            print(f"  â€¢ ë©”ëª¨ë¦¬ ë§µ ì„ê³„ê°’: {self.opt_config.memmap_threshold:,}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ìµœì í™”ëœ ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def upsert_points_batch(
        self, 
        points: List[PointStruct], 
        batch_size: Optional[int] = None,
        show_progress: bool = True
    ) -> Dict[str, Any]:
        """ëŒ€ëŸ‰ í¬ì¸íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì—…ë¡œë“œ"""
        batch_size = batch_size or self.opt_config.batch_size
        total_points = len(points)
        
        if total_points == 0:
            return {"status": "success", "total_points": 0, "batches": 0}
        
        client = await self.get_async_client()
        operation_ids = []
        
        if show_progress:
            print(f"ğŸ“¦ {total_points:,}ê°œ í¬ì¸íŠ¸ë¥¼ {batch_size}ê°œì”© ë°°ì¹˜ ì—…ë¡œë“œ ì‹œì‘...")
        
        start_time = time.time()
        
        for i in range(0, total_points, batch_size):
            batch = points[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (total_points + batch_size - 1) // batch_size
            
            try:
                result = await client.upsert(
                    collection_name=self.collection_name,
                    wait=True,  # ê° ë°°ì¹˜ ì™„ë£Œ ëŒ€ê¸°
                    points=batch
                )
                operation_ids.append(result.operation_id)
                
                if show_progress:
                    progress = (i + len(batch)) / total_points * 100
                    print(f"  ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì™„ë£Œ ({len(batch)}ê°œ) - {progress:.1f}%")
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_num} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise
        
        elapsed_time = time.time() - start_time
        speed = total_points / elapsed_time if elapsed_time > 0 else 0
        
        if show_progress:
            print(f"âœ… ë°°ì¹˜ ì—…ë¡œë“œ ì™„ë£Œ:")
            print(f"  â€¢ ì´ í¬ì¸íŠ¸: {total_points:,}ê°œ")
            print(f"  â€¢ ì´ ë°°ì¹˜: {len(operation_ids)}ê°œ")
            print(f"  â€¢ ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
            print(f"  â€¢ ì²˜ë¦¬ ì†ë„: {speed:.2f} í¬ì¸íŠ¸/ì´ˆ")
        
        return {
            "status": "success",
            "total_points": total_points,
            "batches": len(operation_ids),
            "operation_ids": operation_ids,
            "processing_time": elapsed_time,
            "points_per_second": speed
        }
    
    async def get_collection_stats(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ í†µê³„ ë° ìµœì í™” ìƒíƒœ ì¡°íšŒ"""
        try:
            client = await self.get_async_client()
            
            # ê¸°ë³¸ ì •ë³´
            info = await client.get_collection(collection_name=self.collection_name)
            count = await client.count(collection_name=self.collection_name)
            
            stats = {
                "collection_name": self.collection_name,
                "total_points": count.count,
                "vector_size": info.config.params.vectors.size,
                "distance": info.config.params.vectors.distance.value,
                "status": info.status.value,
                "optimizer_status": info.optimizer_status,
                "indexing_threshold": info.config.optimizer_config.indexing_threshold,
                "segments_count": len(info.result.segments) if info.result and info.result.segments else 0,
                "disk_usage_bytes": sum(seg.disk_usage_bytes for seg in info.result.segments) if info.result and info.result.segments else 0,
                "ram_usage_bytes": sum(seg.ram_usage_bytes for seg in info.result.segments) if info.result and info.result.segments else 0,
            }
            
            # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´
            if info.result and info.result.segments:
                segments_info = []
                for seg in info.result.segments:
                    segments_info.append({
                        "segment_type": seg.segment_type.value if seg.segment_type else "unknown",
                        "num_vectors": seg.num_vectors,
                        "num_points": seg.num_points,
                        "disk_usage_mb": seg.disk_usage_bytes / (1024 * 1024) if seg.disk_usage_bytes else 0,
                        "ram_usage_mb": seg.ram_usage_bytes / (1024 * 1024) if seg.ram_usage_bytes else 0,
                    })
                stats["segments"] = segments_info
            
            return stats
            
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise
    
    async def optimize_collection(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ ìˆ˜ë™ ìµœì í™” ì‹¤í–‰"""
        try:
            print(f"ğŸ”§ ì»¬ë ‰ì…˜ '{self.collection_name}' ìµœì í™” ì‹œì‘...")
            
            client = await self.get_async_client()
            
            # ì¸ë±ìŠ¤ ìµœì í™” ì‹¤í–‰
            result = await client.create_field_index(
                collection_name=self.collection_name,
                field_name="uid",  # uid í•„ë“œì— ì¸ë±ìŠ¤ ìƒì„±
                field_schema=models.PayloadSchemaType.INTEGER
            )
            
            print(f"âœ… ì»¬ë ‰ì…˜ ìµœì í™” ì™„ë£Œ")
            return {"status": "success", "result": result}
            
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ìµœì í™” ì‹¤íŒ¨: {e}")
            raise

async def test_optimization():
    """ìµœì í™” í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Qdrant ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # ìµœì í™”ëœ ê´€ë¦¬ì ìƒì„±
    config = QdrantOptimizationConfig(
        indexing_threshold=50000,  # 12,873ê°œë³´ë‹¤ ì¶©ë¶„íˆ í° ê°’
        batch_size=100,
        default_segment_number=4,
    )
    
    manager = OptimizedQdrantManager(config)
    
    # ìµœì í™”ëœ ì»¬ë ‰ì…˜ ìƒì„±
    await manager.recreate_optimized_collection()
    
    # ì»¬ë ‰ì…˜ í†µê³„ í™•ì¸
    stats = await manager.get_collection_stats()
    print(f"\nğŸ“Š ì»¬ë ‰ì…˜ í†µê³„:")
    print(f"  â€¢ ìƒíƒœ: {stats['status']}")
    print(f"  â€¢ í¬ì¸íŠ¸ ìˆ˜: {stats['total_points']:,}")
    print(f"  â€¢ ë²¡í„° í¬ê¸°: {stats['vector_size']}")
    print(f"  â€¢ ì¸ë±ì‹± ì„ê³„ê°’: {stats['indexing_threshold']:,}")
    print(f"  â€¢ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {stats['segments_count']}")
    print(f"  â€¢ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {stats['disk_usage_bytes'] / (1024*1024):.1f}MB")
    print(f"  â€¢ RAM ì‚¬ìš©ëŸ‰: {stats['ram_usage_bytes'] / (1024*1024):.1f}MB")
    
    print(f"\nâœ… ìµœì í™” ì™„ë£Œ! ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì¤€ë¹„ë¨")

if __name__ == "__main__":
    asyncio.run(test_optimization()) 