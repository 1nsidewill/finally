# benchmark_batch_sizes.py

"""
Redis í ë°°ì¹˜ í¬ê¸° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬
ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ ì²˜ë¦¬ëŸ‰ê³¼ ì§€ì—° ì‹œê°„ì„ ì¸¡ì •í•˜ì—¬ ìµœì ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.
"""

import asyncio
import logging
import time
import statistics
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from datetime import datetime
import json

from src.config import get_settings
from src.database.redis import redis_manager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    batch_size: int
    throughput_per_second: float
    avg_latency_ms: float
    min_latency_ms: float
    max_latency_ms: float
    p95_latency_ms: float
    total_jobs_processed: int
    total_time_seconds: float
    cpu_efficiency: float

class BatchSizeBenchmark:
    """ë°°ì¹˜ í¬ê¸° ë²¤ì¹˜ë§ˆí‚¹ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.settings = get_settings()
        self.test_queue = "benchmark_queue"
        
    async def setup(self):
        """ë²¤ì¹˜ë§ˆí¬ í™˜ê²½ ì„¤ì •"""
        logger.info("ğŸ”§ ë²¤ì¹˜ë§ˆí¬ í™˜ê²½ ì„¤ì • ì¤‘...")
        
        # Redis ì—°ê²° í…ŒìŠ¤íŠ¸
        health = await redis_manager.health_check()
        if not health:
            raise Exception("Redis ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        # í…ŒìŠ¤íŠ¸ í ì´ˆê¸°í™”
        await redis_manager.clear_queue(self.test_queue)
        logger.info("âœ… ë²¤ì¹˜ë§ˆí¬ í™˜ê²½ ì„¤ì • ì™„ë£Œ")
    
    async def cleanup(self):
        """ë²¤ì¹˜ë§ˆí¬ í™˜ê²½ ì •ë¦¬"""
        logger.info("ğŸ§¹ ë²¤ì¹˜ë§ˆí¬ í™˜ê²½ ì •ë¦¬ ì¤‘...")
        await redis_manager.clear_queue(self.test_queue)
        logger.info("âœ… ë²¤ì¹˜ë§ˆí¬ í™˜ê²½ ì •ë¦¬ ì™„ë£Œ")
    
    def generate_test_jobs(self, count: int) -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ìš© ì‘ì—… ë°ì´í„° ìƒì„±"""
        jobs = []
        for i in range(count):
            job = {
                "type": "sync",
                "product_id": i + 1,
                "action": "index",
                "timestamp": datetime.now().isoformat(),
                "metadata": {
                    "source": "benchmark",
                    "batch_test": True,
                    "job_sequence": i
                }
            }
            jobs.append(job)
        return jobs
    
    async def populate_queue(self, job_count: int):
        """íì— í…ŒìŠ¤íŠ¸ ì‘ì—…ë“¤ ì±„ìš°ê¸°"""
        logger.info(f"ğŸ“¥ íì— {job_count}ê°œ ì‘ì—… ì¶”ê°€ ì¤‘...")
        
        jobs = self.generate_test_jobs(job_count)
        batch_size = 100  # í ì±„ìš°ê¸°ìš© ë°°ì¹˜ í¬ê¸°
        
        total_added = 0
        for i in range(0, len(jobs), batch_size):
            batch = jobs[i:i + batch_size]
            added = await redis_manager.push_jobs_batch(batch, self.test_queue)
            total_added += added
        
        logger.info(f"âœ… ì´ {total_added}ê°œ ì‘ì—… ì¶”ê°€ ì™„ë£Œ")
        return total_added
    
    async def process_jobs_with_batch_size(self, batch_size: int, max_jobs: int = 1000) -> BenchmarkResult:
        """íŠ¹ì • ë°°ì¹˜ í¬ê¸°ë¡œ ì‘ì—… ì²˜ë¦¬ ë° ì„±ëŠ¥ ì¸¡ì •"""
        logger.info(f"âš¡ ë°°ì¹˜ í¬ê¸° {batch_size}ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        latencies = []
        total_processed = 0
        start_time = time.time()
        
        while total_processed < max_jobs:
            batch_start = time.time()
            
            # ë°°ì¹˜ë¡œ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
            if batch_size == 1:
                # ë‹¨ì¼ ì‘ì—… ì²˜ë¦¬ (ë¸”ë¡œí‚¹)
                job = await redis_manager.pop_job(self.test_queue, timeout=1)
                jobs = [job] if job else []
            else:
                # ë°°ì¹˜ ì‘ì—… ì²˜ë¦¬ (ë…¼ë¸”ë¡œí‚¹)
                jobs = await redis_manager.pop_jobs_batch(batch_size, self.test_queue)
            
            batch_end = time.time()
            
            if not jobs:
                logger.debug("íê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì¢…ë£Œ.")
                break
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
            batch_latency = (batch_end - batch_start) * 1000  # ms ë‹¨ìœ„
            latencies.append(batch_latency)
            total_processed += len(jobs)
            
            # ì‹¤ì œ ì‘ì—… ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ (ì„ë² ë”©, Qdrant ì €ì¥ ë“±)
            await self.simulate_job_processing(jobs)
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            if total_processed % 100 == 0:
                logger.debug(f"ì²˜ë¦¬ ì§„í–‰: {total_processed}/{max_jobs}")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        if latencies:
            avg_latency = statistics.mean(latencies)
            min_latency = min(latencies)
            max_latency = max(latencies)
            p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95th percentile
        else:
            avg_latency = min_latency = max_latency = p95_latency = 0
        
        throughput = total_processed / total_time if total_time > 0 else 0
        cpu_efficiency = total_processed / (total_time * batch_size) if total_time > 0 else 0
        
        result = BenchmarkResult(
            batch_size=batch_size,
            throughput_per_second=throughput,
            avg_latency_ms=avg_latency,
            min_latency_ms=min_latency,
            max_latency_ms=max_latency,
            p95_latency_ms=p95_latency,
            total_jobs_processed=total_processed,
            total_time_seconds=total_time,
            cpu_efficiency=cpu_efficiency
        )
        
        logger.info(f"âœ… ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {throughput:.2f} jobs/sec")
        return result
    
    async def simulate_job_processing(self, jobs: List[Dict[str, Any]]):
        """ì‹¤ì œ ì‘ì—… ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜"""
        # Redis íì—ì„œ ì‘ì—…ì„ ê°€ì ¸ì˜¨ í›„ì˜ ì‹¤ì œ ì²˜ë¦¬ë¥¼ ì‹œë®¬ë ˆì´ì…˜
        # (í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬, ì„ë² ë”© ìƒì„±, Qdrant ì €ì¥ ë“±)
        
        # ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì²˜ë¦¬ ì‹œê°„ì„ ì‹œë®¬ë ˆì´ì…˜
        # - í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ~1ms per job
        # - ì„ë² ë”© API í˜¸ì¶œ: ~50ms per batch (OpenAI)
        # - Qdrant ì €ì¥: ~10ms per batch
        
        processing_time = 0.001 * len(jobs)  # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œê°„
        processing_time += 0.05  # ì„ë² ë”© API í˜¸ì¶œ ì‹œê°„ (ë°°ì¹˜ë‹¹)
        processing_time += 0.01  # Qdrant ì €ì¥ ì‹œê°„ (ë°°ì¹˜ë‹¹)
        
        await asyncio.sleep(processing_time)
    
    async def run_benchmark_suite(self, 
                                 batch_sizes: List[int] = None,
                                 jobs_per_test: int = 1000) -> List[BenchmarkResult]:
        """ì „ì²´ ë°°ì¹˜ í¬ê¸° ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
        
        if batch_sizes is None:
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë°°ì¹˜ í¬ê¸°ë“¤
            batch_sizes = [1, 5, 10, 20, 30, 50, 75, 100, 150, 200]
        
        logger.info(f"ğŸš€ ë°°ì¹˜ í¬ê¸° ë²¤ì¹˜ë§ˆí¬ ì‹œì‘!")
        logger.info(f"í…ŒìŠ¤íŠ¸í•  ë°°ì¹˜ í¬ê¸°ë“¤: {batch_sizes}")
        logger.info(f"ê° í…ŒìŠ¤íŠ¸ë‹¹ ì‘ì—… ìˆ˜: {jobs_per_test}")
        
        results = []
        
        for batch_size in batch_sizes:
            try:
                # í ì´ˆê¸°í™” ë° ì‘ì—… ì¶”ê°€
                await redis_manager.clear_queue(self.test_queue)
                await self.populate_queue(jobs_per_test + 100)  # ì—¬ìœ ë¶„ ì¶”ê°€
                
                # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
                result = await self.process_jobs_with_batch_size(batch_size, jobs_per_test)
                results.append(result)
                
                # í…ŒìŠ¤íŠ¸ ê°„ ì ì‹œ ëŒ€ê¸°
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                continue
        
        return results
    
    def analyze_results(self, results: List[BenchmarkResult]) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„"""
        if not results:
            return {"error": "ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ìµœê³  ì²˜ë¦¬ëŸ‰ ì°¾ê¸°
        best_throughput = max(results, key=lambda r: r.throughput_per_second)
        
        # ìµœì € ì§€ì—°ì‹œê°„ ì°¾ê¸°
        best_latency = min(results, key=lambda r: r.avg_latency_ms)
        
        # ìµœê³  CPU íš¨ìœ¨ì„± ì°¾ê¸°
        best_efficiency = max(results, key=lambda r: r.cpu_efficiency)
        
        # ê· í˜•ì  ì°¾ê¸° (ì²˜ë¦¬ëŸ‰ê³¼ ì§€ì—°ì‹œê°„ì˜ ê· í˜•)
        # ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚°: (ì²˜ë¦¬ëŸ‰ ì ìˆ˜ - ì§€ì—°ì‹œê°„ íŒ¨ë„í‹°)
        max_throughput = max(r.throughput_per_second for r in results)
        min_latency = min(r.avg_latency_ms for r in results)
        max_latency = max(r.avg_latency_ms for r in results)
        
        for result in results:
            throughput_score = result.throughput_per_second / max_throughput
            latency_penalty = (result.avg_latency_ms - min_latency) / (max_latency - min_latency) if max_latency > min_latency else 0
            result.balance_score = throughput_score - (latency_penalty * 0.3)
        
        best_balance = max(results, key=lambda r: r.balance_score)
        
        analysis = {
            "total_tests": len(results),
            "best_throughput": {
                "batch_size": best_throughput.batch_size,
                "throughput": best_throughput.throughput_per_second,
                "latency": best_throughput.avg_latency_ms
            },
            "best_latency": {
                "batch_size": best_latency.batch_size,
                "throughput": best_latency.throughput_per_second,
                "latency": best_latency.avg_latency_ms
            },
            "best_efficiency": {
                "batch_size": best_efficiency.batch_size,
                "throughput": best_efficiency.throughput_per_second,
                "efficiency": best_efficiency.cpu_efficiency
            },
            "recommended_balance": {
                "batch_size": best_balance.batch_size,
                "throughput": best_balance.throughput_per_second,
                "latency": best_balance.avg_latency_ms,
                "balance_score": best_balance.balance_score
            }
        }
        
        return analysis
    
    def print_results(self, results: List[BenchmarkResult], analysis: Dict[str, Any]):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print("ğŸ“Š ë°°ì¹˜ í¬ê¸° ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
        print(f"{'='*80}")
        
        # ê²°ê³¼ í…Œì´ë¸”
        print(f"{'ë°°ì¹˜í¬ê¸°':<8} {'ì²˜ë¦¬ëŸ‰(jobs/s)':<15} {'í‰ê· ì§€ì—°(ms)':<12} {'P95ì§€ì—°(ms)':<11} {'íš¨ìœ¨ì„±':<8}")
        print("-" * 80)
        
        for result in results:
            print(f"{result.batch_size:<8} "
                  f"{result.throughput_per_second:<15.2f} "
                  f"{result.avg_latency_ms:<12.2f} "
                  f"{result.p95_latency_ms:<11.2f} "
                  f"{result.cpu_efficiency:<8.3f}")
        
        # ë¶„ì„ ê²°ê³¼
        print(f"\n{'='*80}")
        print("ğŸ¯ ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­")
        print(f"{'='*80}")
        
        print(f"ğŸ† ìµœê³  ì²˜ë¦¬ëŸ‰: ë°°ì¹˜í¬ê¸° {analysis['best_throughput']['batch_size']} "
              f"({analysis['best_throughput']['throughput']:.2f} jobs/s)")
        
        print(f"âš¡ ìµœì € ì§€ì—°ì‹œê°„: ë°°ì¹˜í¬ê¸° {analysis['best_latency']['batch_size']} "
              f"({analysis['best_latency']['latency']:.2f} ms)")
        
        print(f"âš™ï¸ ìµœê³  íš¨ìœ¨ì„±: ë°°ì¹˜í¬ê¸° {analysis['best_efficiency']['batch_size']} "
              f"(íš¨ìœ¨ì„±: {analysis['best_efficiency']['efficiency']:.3f})")
        
        print(f"\nğŸ¯ **ê¶Œì¥ ë°°ì¹˜ í¬ê¸°: {analysis['recommended_balance']['batch_size']}**")
        print(f"   - ì²˜ë¦¬ëŸ‰: {analysis['recommended_balance']['throughput']:.2f} jobs/s")
        print(f"   - í‰ê·  ì§€ì—°ì‹œê°„: {analysis['recommended_balance']['latency']:.2f} ms")
        print(f"   - ê· í˜• ì ìˆ˜: {analysis['recommended_balance']['balance_score']:.3f}")
        
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        recommended_size = analysis['recommended_balance']['batch_size']
        print(f"   - REDIS_BATCH_SIZEë¥¼ {recommended_size}ë¡œ ì„¤ì •í•˜ì„¸ìš”")
        print(f"   - ì´ ê°’ì€ ì²˜ë¦¬ëŸ‰ê³¼ ì§€ì—°ì‹œê°„ì˜ ìµœì  ê· í˜•ì ì…ë‹ˆë‹¤")
        
        if recommended_size <= 10:
            print("   - ì‘ì€ ë°°ì¹˜ í¬ê¸°: ì‹¤ì‹œê°„ì„± ì¤‘ì‹œ, ë‚®ì€ ì§€ì—°ì‹œê°„")
        elif recommended_size <= 50:
            print("   - ì¤‘ê°„ ë°°ì¹˜ í¬ê¸°: ì²˜ë¦¬ëŸ‰ê³¼ ì§€ì—°ì‹œê°„ì˜ ê· í˜•")
        else:
            print("   - í° ë°°ì¹˜ í¬ê¸°: ë†’ì€ ì²˜ë¦¬ëŸ‰, ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„±")

async def main():
    """ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í•¨ìˆ˜"""
    benchmark = BatchSizeBenchmark()
    
    try:
        # í™˜ê²½ ì„¤ì •
        await benchmark.setup()
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        # ì‚¬ìš©ì ì •ì˜ ë°°ì¹˜ í¬ê¸°ë“¤ (í˜„ì¬ ì„¤ì •ê°’ í¬í•¨)
        current_batch_size = get_settings().REDIS_BATCH_SIZE
        test_sizes = [1, 5, 10, 15, 20, 25, 30, 40, 50, 75, 100]
        
        # í˜„ì¬ ì„¤ì •ê°’ì´ í…ŒìŠ¤íŠ¸ ëª©ë¡ì— ì—†ìœ¼ë©´ ì¶”ê°€
        if current_batch_size not in test_sizes:
            test_sizes.append(current_batch_size)
            test_sizes.sort()
        
        print(f"í˜„ì¬ ì„¤ì •ëœ REDIS_BATCH_SIZE: {current_batch_size}")
        print(f"í…ŒìŠ¤íŠ¸í•  ë°°ì¹˜ í¬ê¸°ë“¤: {test_sizes}")
        
        results = await benchmark.run_benchmark_suite(
            batch_sizes=test_sizes,
            jobs_per_test=500  # í…ŒìŠ¤íŠ¸ ê·œëª¨ (í”„ë¡œë•ì…˜ì—ì„œëŠ” ë” í¬ê²Œ)
        )
        
        # ê²°ê³¼ ë¶„ì„
        analysis = benchmark.analyze_results(results)
        
        # ê²°ê³¼ ì¶œë ¥
        benchmark.print_results(results, analysis)
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"batch_size_benchmark_{timestamp}.json"
        
        results_data = {
            "timestamp": datetime.now().isoformat(),
            "current_config": current_batch_size,
            "test_sizes": test_sizes,
            "results": [
                {
                    "batch_size": r.batch_size,
                    "throughput_per_second": r.throughput_per_second,
                    "avg_latency_ms": r.avg_latency_ms,
                    "p95_latency_ms": r.p95_latency_ms,
                    "cpu_efficiency": r.cpu_efficiency,
                    "total_jobs_processed": r.total_jobs_processed,
                    "total_time_seconds": r.total_time_seconds
                }
                for r in results
            ],
            "analysis": analysis
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise
    
    finally:
        # í™˜ê²½ ì •ë¦¬
        await benchmark.cleanup()
        await redis_manager.close()

if __name__ == "__main__":
    asyncio.run(main())