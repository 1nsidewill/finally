{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Setup Project Repository",
        "description": "Initialize the project repository with necessary configurations and environment settings.",
        "details": "Create a new Git repository and set up the project structure using FastAPI. Define Docker Compose services for the indexer and Redis. Use Pydantic for environment variable management and configure Alembic for database migrations, particularly for the failed_operations table.",
        "testStrategy": "Verify the repository setup by running Docker Compose to ensure all services start correctly and environment variables are loaded properly.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Establish Database Connections",
        "description": "Implement production-ready connections to PostgreSQL and Qdrant databases.",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "Analyze the existing connection code in test_data_loader.py and refactor it to use a reusable database connection module. Manage connection settings using get_settings() from config.py. Create separate modules for PostgreSQL and Qdrant connections: src/database/postgresql.py for PostgreSQL connection and session management, and src/database/qdrant.py for Qdrant client and collection management. Ensure connection pooling and error handling are implemented.",
        "testStrategy": "Write unit tests to verify the functionality of the new database connection modules, ensuring successful connections and proper error handling. Refactor test_data_loader.py to use the new modules and validate its functionality.",
        "subtasks": [
          {
            "id": 3,
            "title": "Analyze existing connection code in test_data_loader.py",
            "description": "Review the current implementation of database connections in test_data_loader.py to understand the existing setup.",
            "status": "done",
            "details": "<info added on 2025-06-17T05:17:09.035Z>\nê¸°ì¡´ `test_data_loader.py` ë¶„ì„ ê²°ê³¼ì— ë”°ë¼ PostgreSQL ì—°ê²° ëª¨ë“ˆì„ êµ¬í˜„í•  ë•Œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤:\n\n1. **ì—°ê²° í’€ë§ êµ¬í˜„**: ë§¤ë²ˆ ìƒˆë¡œìš´ ì—°ê²°ì„ ìƒì„±í•˜ëŠ” ëŒ€ì‹  ì—°ê²° í’€ë§ì„ í†µí•´ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.\n2. **ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”**: ì—°ê²° ë° ë°ì´í„° ì²˜ë¦¬ ì¤‘ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì—ëŸ¬ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n3. **ëª¨ë“ˆ ì¬ì‚¬ìš©ì„± í–¥ìƒ**: PostgreSQL ì—°ê²°ì„ ìœ„í•œ ëª¨ë“ˆì„ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ì„¤ê³„í•˜ì—¬ ë‹¤ë¥¸ ë¶€ë¶„ì—ì„œë„ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n</info added on 2025-06-17T05:17:09.035Z>"
          },
          {
            "id": 4,
            "title": "Implement PostgreSQL connection module",
            "description": "Create src/database/postgresql.py to manage PostgreSQL connections and sessions, using get_settings() for configuration.",
            "status": "done",
            "details": "<info added on 2025-06-17T05:19:42.431Z>\nPostgreSQL ì—°ê²° ëª¨ë“ˆ êµ¬í˜„ ì™„ë£Œ!\n\n**êµ¬í˜„ëœ ê¸°ëŠ¥:**\nâœ… src/database/postgresql.py - PostgreSQL ì—°ê²° ê´€ë¦¬ì\n- ì—°ê²° í’€ë§ (min_size=5, max_size=20)\n- Context Manager ë°©ì‹ì˜ ì•ˆì „í•œ ì—°ê²° ê´€ë¦¬\n- ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹…\n- í”„ë¡œë•ì…˜ìš© ì¿¼ë¦¬ ë©”ì„œë“œë“¤ (execute_query, execute_command, execute_batch)\n- ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë©”ì„œë“œë“¤ (get_products_by_conversion_status, update_conversion_status ë“±)\n- í—¬ìŠ¤ì²´í¬ ê¸°ëŠ¥\n\n**í•µì‹¬ ê°œì„ ì‚¬í•­:**\n- ê¸°ì¡´: ë§¤ë²ˆ ìƒˆ ì—°ê²° ìƒì„± â†’ ê°œì„ : ì—°ê²° í’€ë§ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ\n- ê¸°ì¡´: ê¸°ë³¸ì ì¸ ì—ëŸ¬ ì²˜ë¦¬ â†’ ê°œì„ : êµ¬ì¡°í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬ ë° ë¡œê¹…\n- ê¸°ì¡´: í•¨ìˆ˜ ë‚´ í•˜ë“œì½”ë”© â†’ ê°œì„ : ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆí™”\n\në‹¤ìŒ: Qdrant ì—°ê²° ëª¨ë“ˆ êµ¬í˜„ ì™„ë£Œë¡œ ì§„í–‰\n</info added on 2025-06-17T05:19:42.431Z>"
          },
          {
            "id": 5,
            "title": "Implement Qdrant connection module",
            "description": "Create src/database/qdrant.py to manage Qdrant client and collections, using get_settings() for configuration.",
            "status": "done",
            "details": "<info added on 2025-06-17T05:20:02.401Z>\nQdrant ì—°ê²° ëª¨ë“ˆ êµ¬í˜„ ì™„ë£Œ!\n\n**êµ¬í˜„ëœ ê¸°ëŠ¥:**\nâœ… src/database/qdrant.py - Qdrant ì—°ê²° ê´€ë¦¬ì\n- ë™ê¸°/ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬ (Lazy Loading)\n- OpenAI ì„ë² ë”© ëª¨ë¸ + ìºì‹± ì§€ì›\n- ì»¬ë ‰ì…˜ ìë™ ìƒì„± (COSINE distance, HNSW ì¸ë±ìŠ¤)\n- í”„ë¡œë•ì…˜ìš© ë©”ì„œë“œë“¤ (upsert_points, delete_points, search_points)\n- ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì§€ì›\n- í—¬ìŠ¤ì²´í¬ ê¸°ëŠ¥\n\n**ê¸°ì¡´ ëŒ€ë¹„ ê°œì„ ì‚¬í•­:**\n- ê¸°ì¡´: qdrant_service ëª¨ë“ˆ ì§ì ‘ ì‚¬ìš© â†’ ê°œì„ : êµ¬ì¡°í™”ëœ ì—°ê²° ê´€ë¦¬ì\n- ê¸°ì¡´: ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì‹œ ì—°ê²° â†’ ê°œì„ : Lazy Loadingìœ¼ë¡œ í•„ìš”ì‹œì—ë§Œ ì—°ê²°\n- ê¸°ì¡´: ë‹¨ì¼ í´ë¼ì´ì–¸íŠ¸ â†’ ê°œì„ : ë™ê¸°/ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ë¶„ë¦¬ ê´€ë¦¬\n- ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í¸ì˜ í•¨ìˆ˜ë“¤ ì œê³µ\n\nâœ… src/database/__init__.py - ëª¨ë“ˆ íŒ¨í‚¤ì§€ êµ¬ì„± ì™„ë£Œ\n\në‹¤ìŒ: ì—°ê²° í’€ë§ ë° ì—ëŸ¬ í•¸ë“¤ë§ ê²€ì¦ìœ¼ë¡œ ì§„í–‰\n</info added on 2025-06-17T05:20:02.401Z>"
          },
          {
            "id": 6,
            "title": "Add connection pooling and error handling",
            "description": "Ensure that connection pooling is set up and error handling is implemented for both PostgreSQL and Qdrant connections.",
            "status": "done"
          },
          {
            "id": 7,
            "title": "Refactor test_data_loader.py to use new modules",
            "description": "Update test_data_loader.py to utilize the new database connection modules for PostgreSQL and Qdrant.",
            "status": "done",
            "details": "<info added on 2025-06-17T05:31:50.713Z>\nê¸°ì¡´ ì½”ë“œ ì •ë¦¬ ë° ë¦¬íŒ©í„°ë§ ì™„ë£Œ!\n\n**ì‚­ì œëœ íŒŒì¼ë“¤:**\nâœ… src/services/test_data_loader.py - ì‚­ì œ ì™„ë£Œ\nâœ… src/services/qdrant_service.py - ì‚­ì œ ì™„ë£Œ (database/qdrant.pyë¡œ ëŒ€ì²´)\n\n**ì •ë¦¬ëœ íŒŒì¼ë“¤:**\nâœ… src/api/router.py - ê¸°ì¡´ ë‚´ìš© ì‚­ì œí•˜ê³  indexerìš© ê¸°ë³¸ êµ¬ì¡°ë¡œ ì¬ì‘ì„±\n- í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ë§Œ ìœ ì§€\n- Redis Queue ê¸°ë°˜ sync ì—”ë“œí¬ì¸íŠ¸ë“¤ TODOë¡œ ì¤€ë¹„\n\n**ê²°ê³¼:**\n- ë¶ˆí•„ìš”í•œ ë ˆê±°ì‹œ ì½”ë“œ ì œê±° ì™„ë£Œ\n- ìƒˆë¡œìš´ database ëª¨ë“ˆ ê¸°ë°˜ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬\n- ë‹¤ìŒ ë‹¨ê³„ (Embedding Service êµ¬í˜„)ë¥¼ ìœ„í•œ ì¤€ë¹„ ì™„ë£Œ\n\nì‘ì—… 2 ì™„ë£Œ ì¤€ë¹„ë¨!\n</info added on 2025-06-17T05:31:50.713Z>"
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Embedding Service",
        "description": "Develop the service to generate embeddings using OpenAI's API.",
        "details": "Create a service that preprocesses text data and generates embeddings using OpenAI's o3 large model. Implement rate limiting and error handling strategies, such as exponential backoff for API calls.",
        "testStrategy": "Test the embedding service with sample data to ensure embeddings are generated correctly and handle API rate limits effectively.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ ê°œë°œ",
            "description": "ë§¤ë¬¼ ì œëª©, ì—°ì‹, ê°€ê²©, í‚¤ë¡œìˆ˜, ë³¸ë¬¸ ë‚´ìš©ì„ ê²°í•©í•˜ì—¬ ì„ë² ë”©ì— ì í•©í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ì „ì²˜ë¦¬ ëª¨ë“ˆì„ ê°œë°œí•©ë‹ˆë‹¤.",
            "dependencies": [],
            "details": "ê° ë§¤ë¬¼ì˜ ë‹¤ì–‘í•œ ì†ì„±ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ê³ , ë¶ˆí•„ìš”í•œ ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì œê±°í•˜ì—¬ OpenAIì˜ ì„ë² ë”© ëª¨ë¸ì— ì…ë ¥í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.",
            "status": "done",
            "testStrategy": "ë‹¤ì–‘í•œ ë§¤ë¬¼ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ì¼ê´€ë˜ê³  ì •í™•í•œì§€ í™•ì¸í•©ë‹ˆë‹¤."
          },
          {
            "id": 2,
            "title": "OpenAI ì„ë² ë”© ì„œë¹„ìŠ¤ í†µí•©",
            "description": "OpenAIì˜ 'text-embedding-3-large' ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.",
            "dependencies": [
              1
            ],
            "details": "OpenAIì˜ APIë¥¼ í˜¸ì¶œí•˜ì—¬ 3072ì°¨ì›ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. API í˜¸ì¶œ ì‹œ í•„ìš”í•œ ì¸ì¦ ë° ìš”ì²­ í˜•ì‹ì„ ì¤€ìˆ˜í•©ë‹ˆë‹¤.",
            "status": "done",
            "testStrategy": "ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ê³¼ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤."
          },
          {
            "id": 3,
            "title": "Rate Limiting ë° Exponential Backoff êµ¬í˜„",
            "description": "OpenAI API í˜¸ì¶œ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” Rate Limit ì´ˆê³¼ ë° ì˜¤ë¥˜ ìƒí™©ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ Rate Limitingê³¼ Exponential Backoff ì „ëµì„ êµ¬í˜„í•©ë‹ˆë‹¤.",
            "dependencies": [
              2
            ],
            "details": "API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ëŒ€ê¸° ì‹œê°„ì„ ì ìš©í•˜ì—¬ ì¬ì‹œë„í•˜ë©°, ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì„œë¹„ìŠ¤ì˜ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.",
            "status": "done",
            "testStrategy": "ì˜ë„ì ìœ¼ë¡œ API í˜¸ì¶œ ì‹¤íŒ¨ë¥¼ ìœ ë„í•˜ì—¬ ì¬ì‹œë„ ë¡œì§ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
          },
          {
            "id": 4,
            "title": "ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€",
            "description": "ì—¬ëŸ¬ ê°œì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.",
            "dependencies": [
              2
            ],
            "details": "ì—¬ëŸ¬ ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ìš”ì²­ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ API í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì¤„ì´ê³  íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.",
            "status": "done",
            "testStrategy": "ë‹¤ì–‘í•œ í¬ê¸°ì˜ ë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ê³¼ ì •í™•ë„ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
          },
          {
            "id": 5,
            "title": "ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™",
            "description": "ê¸°ì¡´ì˜ 'database/qdrant.py' ëª¨ë“ˆê³¼ ì—°ê³„í•˜ì—¬ ìƒì„±ëœ ì„ë² ë”©ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ê¸°ëŠ¥ì„ í†µí•©í•©ë‹ˆë‹¤.",
            "dependencies": [
              2,
              4
            ],
            "details": "ìƒì„±ëœ ì„ë² ë”© ë²¡í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê³ , í•„ìš” ì‹œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ê¸°ì¡´ ëª¨ë“ˆê³¼ì˜ ì—°ë™ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n<info added on 2025-06-17T06:15:14.788Z>\në°ì´í„°ë² ì´ìŠ¤ í†µí•© ì™„ë£Œ! ì£¼ìš” ì„±ê³¼:\n\nâœ… UUID ë³€í™˜ ë¬¸ì œ í•´ê²°:\n- ensure_valid_uuid() í•¨ìˆ˜ë¡œ ì •ìˆ˜ IDë¥¼ UUID í˜•ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜\n- uuid.uuid5()ë¥¼ ì‚¬ìš©í•´ ë™ì¼í•œ IDì— ëŒ€í•´ ì¼ê´€ëœ UUID ìƒì„±\n\nâœ… ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆ ì™„ë²½ ì—°ë™:\n- PostgreSQL: get_products_by_conversion_status() ë©”ì„œë“œ í™œìš©\n- Qdrant: ê¸°ì¡´ ë§¤ë‹ˆì €ì˜ ì„ë² ë”© ì„œë¹„ìŠ¤ì™€ ë²¡í„° ì €ì¥ ê¸°ëŠ¥ í™œìš©\n- search_similar_vectors() í˜¸í™˜ì„± ë©”ì„œë“œ ì¶”ê°€\n\nâœ… config.py ê¸°ë°˜ ì„¤ì • í†µí•©:\n- í™˜ê²½ë³€ìˆ˜ë¥¼ config.pyì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •\n- ë°°ì¹˜ ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •ë“¤ì„ config.pyì— optionalë¡œ ì¶”ê°€\n- .env íŒŒì¼ ëŒ€ì‹  config.py ë°©ì‹ìœ¼ë¡œ í†µì¼\n\nâœ… í†µí•© í…ŒìŠ¤íŠ¸ 100% í†µê³¼:\n- 7ê°œ í…ŒìŠ¤íŠ¸ ì „ë¶€ ì„±ê³µ\n- ì„¤ì •, ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°, í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬, ì„ë² ë”© ìƒì„±, PostgreSQL/Qdrant ì—°ì‚°, ë°°ì¹˜ í”„ë¡œì„¸ì„œ ëª¨ë‘ ì •ìƒ ì‘ë™\n\nâœ… ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì™„ì„±:\n- ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì™€ í˜¸í™˜\n- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ â†’ ì„ë² ë”© ìƒì„± â†’ ë²¡í„° ì €ì¥ â†’ PostgreSQL í”Œë˜ê·¸ ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸ ì™„ì„±\n- 30k+ ë§¤ë¬¼ ëŒ€ëŸ‰ ì²˜ë¦¬ ì¤€ë¹„ ì™„ë£Œ\n</info added on 2025-06-17T06:15:14.788Z>",
            "status": "done",
            "testStrategy": "ì„ë² ë”© ìƒì„± í›„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê³ , ì €ì¥ëœ ì„ë² ë”©ì„ ê²€ìƒ‰í•˜ì—¬ ì¼ê´€ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤."
          }
        ]
      },
      {
        "id": 4,
        "title": "Develop Redis Queue Worker",
        "description": "Create a worker to process jobs from the Redis queue asynchronously.",
        "details": "Implement a Redis queue worker using Python's asyncio and aioredis to poll jobs in batches. Ensure the worker can handle sync, update, and delete operations efficiently and log failures to the failed_operations table.",
        "testStrategy": "Simulate job processing by pushing test jobs to the Redis queue and verify that they are processed correctly and failures are logged.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "in-progress",
        "subtasks": [
          {
            "id": 1,
            "title": "ë°°ì¹˜ í¬ê¸° íŠœë‹",
            "description": "Redis íì—ì„œ ì‘ì—…ì„ ì²˜ë¦¬í•  ë•Œ ìµœì ì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ê²°ì •í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.",
            "dependencies": [],
            "details": "ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¥¼ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì²˜ë¦¬ëŸ‰ê³¼ ì§€ì—° ì‹œê°„ì˜ ê· í˜•ì„ ë§ì¶¥ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‘ì€ ë°°ì¹˜ëŠ” ì§€ì—° ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ ì²˜ë¦¬ëŸ‰ì´ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´, í° ë°°ì¹˜ëŠ” ì²˜ë¦¬ëŸ‰ì„ ë†’ì¼ ìˆ˜ ìˆì§€ë§Œ ì§€ì—° ì‹œê°„ì´ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "status": "in-progress",
            "testStrategy": "ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ì—¬ ìµœì ì˜ í¬ê¸°ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."
          },
          {
            "id": 2,
            "title": "ì—°ê²° í’€ ìµœì í™”",
            "description": "aioredisì˜ ì—°ê²° í’€ ì„¤ì •ì„ ì¡°ì •í•˜ì—¬ ë™ì‹œì„± ë° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.",
            "dependencies": [],
            "details": "ì—°ê²° í’€ì˜ ìµœëŒ€ í¬ê¸°ì™€ ì¬ì‚¬ìš© ì „ëµì„ ì¡°ì •í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì—°ê²° í’€ì˜ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ë©´ ë™ì‹œ ì—°ê²°ì„ ë” ë§ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆì§€ë§Œ, ê³¼ë„í•œ í¬ê¸°ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì ì ˆí•œ í¬ê¸°ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
            "status": "pending",
            "testStrategy": "ë‹¤ì–‘í•œ ì—°ê²° í’€ í¬ê¸°ì™€ ì„¤ì •ìœ¼ë¡œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ì—¬ ìµœì ì˜ êµ¬ì„±ì„ ì°¾ìŠµë‹ˆë‹¤."
          },
          {
            "id": 3,
            "title": "ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ëª¨ë‹ˆí„°ë§ êµ¬í˜„",
            "description": "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•´ Prometheusì™€ ê°™ì€ ë„êµ¬ë¥¼ í†µí•©í•˜ì—¬ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.",
            "dependencies": [],
            "details": "ì‘ì—… ì²˜ë¦¬ìœ¨, ì§€ì—° ì‹œê°„, ì˜¤ë¥˜ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•˜ê³  ì‹œê°í™”í•˜ì—¬ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì„±ëŠ¥ ë³‘ëª© ì§€ì ì„ ì‹ë³„í•˜ê³  ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "status": "pending",
            "testStrategy": "ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ì„±ëŠ¥ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ , í•„ìš”ì— ë”°ë¼ ì¡°ì •ì„ í•©ë‹ˆë‹¤."
          },
          {
            "id": 4,
            "title": "ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê³„íš ë° ì‹¤í–‰",
            "description": "ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›Œì»¤ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ë³‘ëª© ì§€ì ì„ ì‹ë³„í•©ë‹ˆë‹¤.",
            "dependencies": [],
            "details": "Locustì™€ ê°™ì€ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì›Œì»¤ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë³‘ëª© ì§€ì ì„ ì‹ë³„í•˜ê³  ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "status": "pending",
            "testStrategy": "ë‹¤ì–‘í•œ ë¶€í•˜ ì¡°ê±´ì—ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì„±ëŠ¥ ê°œì„  ë°©ì•ˆì„ ë„ì¶œí•©ë‹ˆë‹¤."
          },
          {
            "id": 5,
            "title": "ì‹¤íŒ¨í•œ ì‘ì—… ë¡œê¹… ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„",
            "description": "ì‹¤íŒ¨í•œ ì‘ì—…ì„ ë¡œê¹…í•˜ê³  ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•˜ì—¬ ì‹ ë¢°ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.",
            "dependencies": [],
            "details": "ì‹¤íŒ¨í•œ ì‘ì—…ì„ 'failed_operations' í…Œì´ë¸”ì— ê¸°ë¡í•˜ê³ , ì¼ì •í•œ ì¬ì‹œë„ ì •ì±…ì„ êµ¬í˜„í•˜ì—¬ ì‘ì—…ì˜ ì‹ ë¢°ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì§€ìˆ˜ ë°±ì˜¤í”„ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„ ê°„ê²©ì„ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n<info added on 2025-06-17T23:57:24.972Z>\nâœ… ì‹¤íŒ¨í•œ ì‘ì—… ë¡œê¹… ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„ ì™„ë£Œ!\n\n## êµ¬í˜„ëœ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ë“¤:\n\n### 1. FailureHandler ì„œë¹„ìŠ¤ (src/services/failure_handler.py)\n- **OperationType** ë° **RetryStrategy** Enum ì •ì˜\n- **FailedOperation** ë°ì´í„°í´ë˜ìŠ¤ë¡œ ì‹¤íŒ¨ ì‘ì—… ì •ë³´ êµ¬ì¡°í™”\n- **ì§€ìˆ˜ ë°±ì˜¤í”„, ì„ í˜• ë°±ì˜¤í”„, ê³ ì • ê°„ê²©** ì¬ì‹œë„ ì „ëµ ì§€ì›\n- ì‹¤íŒ¨ ë¡œê¹…, ì¬ì‹œë„ ê°€ëŠ¥ ì‘ì—… ì¡°íšŒ, ì¬ì‹œë„ ì‹œë„ ì—…ë°ì´íŠ¸ ê¸°ëŠ¥\n- ì˜êµ¬ ì‹¤íŒ¨ í‘œì‹œ, í†µê³„ ì¡°íšŒ, ì˜¤ë˜ëœ ê¸°ë¡ ì •ë¦¬ ê¸°ëŠ¥\n\n### 2. PostgreSQL ìŠ¤í‚¤ë§ˆ (migrations/create_failed_operations_table.sql)\n- **failed_operations** í…Œì´ë¸” ìƒì„± ìŠ¤í¬ë¦½íŠ¸\n- ìµœì í™”ëœ ì¸ë±ìŠ¤: ì¬ì‹œë„ ìŠ¤ì¼€ì¤„ë§, ì œí’ˆ UID, ì‘ì—… íƒ€ì…, ìƒíƒœë³„\n- **failed_operations_stats** ë·°ë¡œ í†µê³„ ì¡°íšŒ í¸ì˜ì„± ì œê³µ\n- **cleanup_resolved_failures()** í•¨ìˆ˜ë¡œ DB ì •ë¦¬ ìë™í™”\n\n### 3. ReliableWorker ë˜í¼ (src/workers/reliable_worker.py)\n- **failure_context** ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ëª¨ë“  ì‘ì—…ì„ ì•ˆì „í•˜ê²Œ ë˜í•‘\n- **safe_sync_operation**, **safe_update_operation**, **safe_delete_operation**, **safe_embedding_operation** ë©”ì„œë“œë“¤\n- **process_failed_operations()** - ì‹¤íŒ¨í•œ ì‘ì—…ë“¤ì˜ ìë™ ì¬ì‹œë„ ì²˜ë¦¬\n- ê° ì‘ì—… íƒ€ì…ë³„ ë§ì¶¤í˜• ì¬ì‹œë„ ë¡œì§ êµ¬í˜„\n- ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì ì ˆí•œ ì§€ì—° ì‹œê°„ ì ìš©\n\n### 4. ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ (test_failure_mechanism.py)\n- ì‹¤íŒ¨ ë¡œê¹…, ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜, ì›Œì»¤ ë˜í•‘, í†µê³„ ì¡°íšŒ, ì„±ê³µ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\n- ì˜ë„ì ìœ¼ë¡œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ ì‹¤íŒ¨ ì²˜ë¦¬ ë¡œì§ ê²€ì¦\n- PostgreSQL ì—°ê²° ì‹¤íŒ¨ ìƒí™©ì—ì„œë„ ì½”ë“œ êµ¬ì¡°ëŠ” ì •ìƒ ì‘ë™ í™•ì¸\n\n## í•µì‹¬ ê¸°ëŠ¥ë“¤:\n\nâœ… **ìë™ ì‹¤íŒ¨ ë¡œê¹…**: ëª¨ë“  ì‘ì—… ì‹¤íŒ¨ ì‹œ ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´ì™€ ì»¨í…ìŠ¤íŠ¸ ì €ì¥\nâœ… **ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„**: 60ì´ˆ â†’ 120ì´ˆ â†’ 240ì´ˆ â†’ ... (ìµœëŒ€ 1ì‹œê°„) ê°„ê²©ìœ¼ë¡œ ì¬ì‹œë„\nâœ… **ì˜êµ¬ ì‹¤íŒ¨ ì²˜ë¦¬**: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ì˜êµ¬ ì‹¤íŒ¨ë¡œ ë§ˆí‚¹\nâœ… **ìƒì„¸ í†µê³„**: ì‘ì—… íƒ€ì…ë³„ ì´ ì‹¤íŒ¨, í•´ê²°, ì˜êµ¬ ì‹¤íŒ¨, ëŒ€ê¸° ì¤‘ ì¬ì‹œë„ ìˆ˜ ì¶”ì \nâœ… **ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´**: ì‹¤íŒ¨ ì‹œì ì˜ ì‘ì—… ìƒí™© ì •ë³´ JSONìœ¼ë¡œ ì €ì¥\nâœ… **ì•ˆì „í•œ ë˜í•‘**: Context Manager íŒ¨í„´ìœ¼ë¡œ ëª¨ë“  ì‘ì—…ì„ ìë™ìœ¼ë¡œ ì‹¤íŒ¨ ì²˜ë¦¬ì™€ ì—°ê²°\n\n## ì‹ ë¢°ì„± í–¥ìƒ íš¨ê³¼:\n- **ì¼ì‹œì  ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜**: OpenAI API íƒ€ì„ì•„ì›ƒ, Qdrant ì—°ê²° ì‹¤íŒ¨ ë“± ìë™ ì¬ì‹œë„\n- **ë°ì´í„° ì •í•©ì„±**: PostgreSQL íŠ¸ëœì­ì…˜ ì‹¤íŒ¨ ì‹œ Qdrant ë¡¤ë°± ì§€ì›\n- **ìš´ì˜ ëª¨ë‹ˆí„°ë§**: ì‹¤íŒ¨ í†µê³„ë¥¼ í†µí•œ ì‹œìŠ¤í…œ ì•ˆì •ì„± ëª¨ë‹ˆí„°ë§\n- **ì¥ì•  ë³µêµ¬**: ì„œë¹„ìŠ¤ ì¬ì‹œì‘ í›„ ë¯¸ì™„ë£Œ ì‘ì—…ë“¤ ìë™ ì¬ì‹œë„\n</info added on 2025-06-17T23:57:24.972Z>\n<info added on 2025-06-18T00:09:43.553Z>\nâœ… ì‹¤íŒ¨í•œ ì‘ì—… ë¡œê¹… ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n\n## í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n\n### ğŸ¯ ì„±ê³µí•œ ê¸°ëŠ¥ë“¤:\n- **config.py í†µí•©**: .env ëŒ€ì‹  config.pyì—ì„œ ì„¤ì •ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œ\n- **PostgreSQL í…Œì´ë¸” ìƒì„±**: failed_operations í…Œì´ë¸”ì´ ì •ìƒ ìƒì„±ë¨\n- **ì‹¤íŒ¨ ë¡œê¹…**: ì˜ë„ì  ì˜ˆì™¸ ë°œìƒ ì‹œ ì‹¤íŒ¨ ì‘ì—…ì´ ì •ìƒì ìœ¼ë¡œ ë¡œê¹…ë¨ (ID: 1, 2)\n- **ì•ˆì •ì ì¸ ì›Œì»¤**: failure_context ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ê°€ ì •ìƒ ì‘ë™\n- **ì‹¤íŒ¨ í†µê³„**: íƒ€ì…ë³„ ì‹¤íŒ¨ í†µê³„ë¥¼ ì •í™•íˆ ì§‘ê³„ ('sync': 2ê°œ ì‹¤íŒ¨, 0ê°œ í•´ê²°, 2ê°œ ì¬ì‹œë„ ëŒ€ê¸°)\n\n### ğŸ“Š í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼:\n```\nsync:\n  ì´ ì‹¤íŒ¨: 2\n  í•´ê²°ë¨: 0  \n  ì˜êµ¬ ì‹¤íŒ¨: 0\n  ì¬ì‹œë„ ëŒ€ê¸°: 2\n  í‰ê·  ì¬ì‹œë„ íšŸìˆ˜: 0.00\n```\n\n### âš™ï¸ ë™ì‘ í™•ì¸:\n- TestExceptionìœ¼ë¡œ ì˜ë„ì  ì˜ˆì™¸ ë°œìƒ â†’ ì‹¤íŒ¨ ë¡œê¹… ì„±ê³µ\n- reliable_worker.failure_context() â†’ ì˜ˆì™¸ ìºì¹˜ ë° ë¡œê¹… ì„±ê³µ\n- failure_handler.get_failure_stats() â†’ í†µê³„ ì¡°íšŒ ì„±ê³µ\n- PostgreSQL ì—°ê²° í’€ ì •ìƒ ìƒì„±/ì¢…ë£Œ\n\n### ğŸ”§ ê°œì„ ì‚¬í•­:\n- ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì˜ next_retry_at ì„¤ì • ë¡œì§ ì¼ë¶€ ì¡°ì • í•„ìš”\n- í•˜ì§€ë§Œ í•µì‹¬ ì‹¤íŒ¨ ë¡œê¹…ê³¼ í†µê³„ ê¸°ëŠ¥ì€ ì™„ì „íˆ ì‘ë™\n\nì „ë°˜ì ìœ¼ë¡œ **ì‹¤íŒ¨ ì²˜ë¦¬ ë©”ì»¤ë‹ˆì¦˜ì´ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„ë˜ê³  í…ŒìŠ¤íŠ¸ë¨**!\n</info added on 2025-06-18T00:09:43.553Z>",
            "status": "done",
            "testStrategy": "ì˜ë„ì ìœ¼ë¡œ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œì¼œ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
          }
        ]
      },
      {
        "id": 5,
        "title": "Build Basic API Endpoints",
        "description": "Create basic API endpoints for synchronization, health checks, and status updates.",
        "details": "Develop endpoints using FastAPI: POST /sync to add sync jobs to the queue, GET /health for service health checks, and GET /status to report processing progress. Implement basic error handling for these endpoints.",
        "testStrategy": "Use tools like Postman to test API endpoints for expected responses and error handling.",
        "priority": "medium",
        "dependencies": [
          1,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement Failure Handling System",
        "description": "Develop a system to log and retry failed indexing operations.",
        "details": "Define the schema for the failed_operations table and implement logic to log failed operations. Create a POST /retry endpoint to manually retry failed jobs, with detailed logging for failure analysis.",
        "testStrategy": "Test the failure logging by forcing errors in the queue worker and verify retry functionality through the API.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Create Monitoring Dashboard",
        "description": "Develop a simple web interface to monitor processing status and errors.",
        "details": "Build a basic HTML and JavaScript dashboard to display processing progress, error rates, and queue status. Integrate with the API to fetch real-time data.",
        "testStrategy": "Test the dashboard by simulating different processing states and ensuring accurate data display.",
        "priority": "low",
        "dependencies": [
          5,
          6
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Optimize Batch Processing",
        "description": "Enhance performance by optimizing batch processing and concurrency settings.",
        "details": "Adjust batch sizes and concurrency levels for the Redis queue worker to optimize memory usage and processing speed. Implement batch upsert operations in Qdrant for efficiency.",
        "testStrategy": "Benchmark processing times with different batch sizes and concurrency settings to identify optimal configurations.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Ensure Data Consistency",
        "description": "Implement mechanisms to ensure data consistency between PostgreSQL and Qdrant.",
        "details": "Use the is_conversion flag in PostgreSQL to track synchronization status and ensure consistency. Implement logic to detect and handle incremental updates based on updated_at timestamps.",
        "testStrategy": "Test data consistency by simulating updates and deletions, ensuring changes are reflected in both databases.",
        "priority": "medium",
        "dependencies": [
          3,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Handle Large Data Volumes",
        "description": "Implement strategies to manage large data volumes efficiently.",
        "details": "Implement checkpointing and progress tracking to handle large data volumes without exceeding memory limits. Ensure the system can resume processing from the last checkpoint in case of failures.",
        "testStrategy": "Test with large datasets to ensure the system can process them without running out of memory and can resume from checkpoints.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Integrate with Scraper Team",
        "description": "Ensure seamless integration with the Scraper team's Redis Queue interface.",
        "details": "Provide clear API documentation and example code for the Scraper team to integrate with the Redis Queue. Ensure compatibility and handle any interface mismatches.",
        "testStrategy": "Coordinate with the Scraper team to test integration and resolve any issues that arise during testing.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Prepare for GCP Migration",
        "description": "Plan and prepare for migrating the system to Google Cloud Platform.",
        "details": "Document the migration process, including setting up GCP VMs and configuring network settings. Ensure all services are compatible with the GCP environment.",
        "testStrategy": "Conduct a test migration to a GCP environment and verify that all services function correctly post-migration.",
        "priority": "low",
        "dependencies": [
          1,
          10
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-06-17T05:06:36.489Z",
      "updated": "2025-06-18T00:11:07.309Z",
      "description": "Tasks for master context"
    }
  }
}