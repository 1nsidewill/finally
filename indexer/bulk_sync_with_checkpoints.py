import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import uuid

# Add project root to path
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from src.config import get_settings
from src.database.postgresql import PostgreSQLManager
from src.database.qdrant import QdrantManager
from src.services.embedding_service import EmbeddingService


class CheckpointManager:
    """Manages checkpoints for bulk synchronization process"""
    
    def __init__(self, checkpoint_file: str = "bulk_sync_checkpoint.json"):
        self.checkpoint_file = Path(checkpoint_file)
        self.data = self._load_checkpoint()
    
    def _load_checkpoint(self) -> Dict:
        """Load existing checkpoint or create new one"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, Exception) as e:
                print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                return self._create_new_checkpoint()
        else:
            return self._create_new_checkpoint()
    
    def _create_new_checkpoint(self) -> Dict:
        """Create a new checkpoint structure"""
        return {
            "session_id": str(uuid.uuid4()),
            "started_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "total_products": 0,
            "processed_count": 0,
            "last_processed_uid": None,
            "batch_size": 100,
            "current_batch": 0,
            "total_batches": 0,
            "status": "initialized",
            "success_count": 0,
            "error_count": 0,
            "batches_completed": [],
            "errors": []
        }
    
    def save_checkpoint(self):
        """Save current checkpoint to file"""
        self.data["last_updated"] = datetime.now().isoformat()
        try:
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨: ë°°ì¹˜ {self.data['current_batch']}/{self.data['total_batches']}")
        except Exception as e:
            print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def update_progress(self, batch_num: int, processed_count: int, success_count: int, 
                       error_count: int, last_uid: Optional[str] = None):
        """Update checkpoint with current progress"""
        self.data.update({
            "current_batch": batch_num,
            "processed_count": processed_count,
            "success_count": success_count,
            "error_count": error_count,
            "last_processed_uid": last_uid,
            "status": "running"
        })
        
        if batch_num not in self.data["batches_completed"]:
            self.data["batches_completed"].append(batch_num)
        
        self.save_checkpoint()
    
    def mark_completed(self):
        """Mark the entire process as completed"""
        self.data.update({
            "status": "completed",
            "completed_at": datetime.now().isoformat()
        })
        self.save_checkpoint()
    
    def mark_failed(self, error_msg: str):
        """Mark the process as failed"""
        self.data.update({
            "status": "failed",
            "failed_at": datetime.now().isoformat()
        })
        self.data["errors"].append({
            "timestamp": datetime.now().isoformat(),
            "error": error_msg
        })
        self.save_checkpoint()


class BulkSynchronizer:
    """Handles bulk synchronization of products with checkpointing"""
    
    def __init__(self, batch_size: int = 100):
        self.config = get_settings()
        self.batch_size = batch_size
        self.checkpoint = CheckpointManager()
        
        # Initialize services
        self.pg_manager = PostgreSQLManager()
        self.qdrant_manager = QdrantManager()
        self.embedding_service = EmbeddingService()
        
        # Statistics
        self.total_products = 0
        self.processed_count = 0
        self.success_count = 0
        self.error_count = 0
    
    async def initialize(self):
        """Initialize all database connections"""
        print("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì´ˆê¸°í™” ì¤‘...")
        try:
            # PostgreSQL connection will be handled per operation
            print("âœ… PostgreSQL ì—°ê²° ì¤€ë¹„ ì™„ë£Œ")
            print("âœ… Qdrant ì—°ê²° ì¤€ë¹„ ì™„ë£Œ")
            print("âœ… OpenAI ì„ë² ë”© ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def get_total_product_count(self) -> int:
        """Get total count of products to process"""
        async with self.pg_manager.get_connection() as conn:
            result = await conn.fetchrow("""
                SELECT COUNT(*) as total 
                FROM product 
                WHERE is_conversion = false AND status = 1
            """)
            return result['total']
    
    async def get_products_batch(self, offset: int, limit: int) -> List[Dict]:
        """Get a batch of products starting from offset"""
        async with self.pg_manager.get_connection() as conn:
            rows = await conn.fetch("""
                SELECT uid, title, content, brand, price, status, is_conversion
                FROM product 
                WHERE is_conversion = false AND status = 1
                ORDER BY uid
                LIMIT $1 OFFSET $2
            """, limit, offset)
            
            return [dict(row) for row in rows]
    
    async def process_single_product(self, product: Dict) -> Dict:
        """Process a single product and return result"""
        try:
            # Create embedding text
            embedding_text = f"{product['title']} {product['content']} ë¸Œëœë“œ:{product['brand']} ê°€ê²©:{product['price']}ì›"
            
            # Generate embedding (ë™ê¸° í•¨ìˆ˜ ì‚¬ìš©)
            embedding_array = self.embedding_service.create_embedding(embedding_text)
            if embedding_array is None:
                raise ValueError("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            
            # Convert numpy array to list
            embedding = embedding_array.tolist()
            
            # Insert into Qdrant using upsert_vector_async
            await self.qdrant_manager.upsert_vector_async(
                vector_id=str(product['uid']),
                vector=embedding,
                metadata={
                    "title": product['title'],
                    "content": product['content'] or "",
                    "brand": product['brand'] or "",
                    "price": float(product['price']) if product['price'] else 0.0,
                    "status": product['status'],
                    "uid": product['uid']
                }
            )
            
            # Update PostgreSQL is_conversion flag
            async with self.pg_manager.get_connection() as conn:
                await conn.execute("""
                    UPDATE product 
                    SET is_conversion = true 
                    WHERE uid = $1
                """, product['uid'])
            
            return {
                "uid": product['uid'],
                "status": "success",
                "error": None
            }
            
        except Exception as e:
            return {
                "uid": product['uid'],
                "status": "error",
                "error": str(e)
            }
    
    async def process_batch(self, batch_num: int, products: List[Dict]) -> Dict:
        """Process a batch of products"""
        batch_start_time = datetime.now()
        batch_results = {
            "batch_num": batch_num,
            "total_items": len(products),
            "success_count": 0,
            "error_count": 0,
            "errors": [],
            "processing_time": 0
        }
        
        print(f"\nğŸ“¦ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì‹œì‘ ({len(products)}ê°œ ì œí’ˆ)")
        
        # Process each product in the batch
        for i, product in enumerate(products):
            try:
                result = await self.process_single_product(product)
                
                if result["status"] == "success":
                    batch_results["success_count"] += 1
                    self.success_count += 1
                    print(f"  âœ… {i+1}/{len(products)} - {product['uid'][:8]}...")
                else:
                    batch_results["error_count"] += 1
                    batch_results["errors"].append(result)
                    self.error_count += 1
                    print(f"  âŒ {i+1}/{len(products)} - {product['uid'][:8]}... ì‹¤íŒ¨: {result['error']}")
                
                self.processed_count += 1
                
                # Small delay to prevent overwhelming the services
                await asyncio.sleep(0.1)
                
            except Exception as e:
                batch_results["error_count"] += 1
                batch_results["errors"].append({
                    "uid": product.get('uid', 'unknown'),
                    "status": "error", 
                    "error": str(e)
                })
                self.error_count += 1
                print(f"  âŒ {i+1}/{len(products)} - ì˜ˆì™¸ ë°œìƒ: {e}")
        
        # Calculate processing time
        batch_results["processing_time"] = (datetime.now() - batch_start_time).total_seconds()
        
        # Update checkpoint
        last_uid = products[-1]['uid'] if products else None
        self.checkpoint.update_progress(
            batch_num, self.processed_count, self.success_count, 
            self.error_count, last_uid
        )
        
        print(f"ğŸ“Š ë°°ì¹˜ {batch_num} ì™„ë£Œ: ì„±ê³µ {batch_results['success_count']}, ì‹¤íŒ¨ {batch_results['error_count']}")
        print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {batch_results['processing_time']:.2f}ì´ˆ")
        
        return batch_results
    
    async def run_bulk_sync(self, resume: bool = True):
        """Run the complete bulk synchronization process"""
        start_time = datetime.now()
        
        print("ğŸš€ ëŒ€ëŸ‰ ë™ê¸°í™” í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼: {self.checkpoint.checkpoint_file}")
        
        # Initialize connections
        if not await self.initialize():
            return False
        
        # Get total product count
        self.total_products = await self.get_total_product_count()
        print(f"ğŸ“Š ì´ ì²˜ë¦¬ ëŒ€ìƒ: {self.total_products:,}ê°œ ì œí’ˆ")
        
        if self.total_products == 0:
            print("âš ï¸ ì²˜ë¦¬í•  ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            return True
        
        # Calculate batches
        total_batches = (self.total_products + self.batch_size - 1) // self.batch_size
        
        # Update checkpoint with initial data
        self.checkpoint.data.update({
            "total_products": self.total_products,
            "batch_size": self.batch_size,
            "total_batches": total_batches
        })
        
        # Resume from checkpoint if requested
        start_batch = 0
        if resume and self.checkpoint.data.get("status") == "running":
            start_batch = self.checkpoint.data.get("current_batch", 0)
            self.processed_count = self.checkpoint.data.get("processed_count", 0)
            self.success_count = self.checkpoint.data.get("success_count", 0)
            self.error_count = self.checkpoint.data.get("error_count", 0)
            print(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘: ë°°ì¹˜ {start_batch}ë¶€í„°")
        
        print(f"ğŸ“¦ ì´ ë°°ì¹˜ ìˆ˜: {total_batches} (ë°°ì¹˜ í¬ê¸°: {self.batch_size})")
        
        try:
            # Process each batch
            for batch_num in range(start_batch, total_batches):
                offset = batch_num * self.batch_size
                
                # Get products for this batch
                products = await self.get_products_batch(offset, self.batch_size)
                
                if not products:
                    print(f"âš ï¸ ë°°ì¹˜ {batch_num}ì—ì„œ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # Process the batch
                batch_result = await self.process_batch(batch_num + 1, products)
                
                # Progress report
                progress_pct = ((batch_num + 1) / total_batches) * 100
                print(f"ğŸ“ˆ ì „ì²´ ì§„í–‰ë¥ : {progress_pct:.1f}% ({batch_num + 1}/{total_batches})")
                print(f"ğŸ“Š ëˆ„ì  í†µê³„: ì„±ê³µ {self.success_count:,}, ì‹¤íŒ¨ {self.error_count:,}")
        
        except Exception as e:
            error_msg = f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            print(f"âŒ {error_msg}")
            self.checkpoint.mark_failed(error_msg)
            return False
        
        # Mark as completed
        self.checkpoint.mark_completed()
        
        # Final statistics
        total_time = (datetime.now() - start_time).total_seconds()
        
        print("\nğŸ‰ ëŒ€ëŸ‰ ë™ê¸°í™” ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… í†µê³„:")
        print(f"  â€¢ ì´ ì²˜ë¦¬: {self.processed_count:,}ê°œ")
        print(f"  â€¢ ì„±ê³µ: {self.success_count:,}ê°œ")
        print(f"  â€¢ ì‹¤íŒ¨: {self.error_count:,}ê°œ")
        print(f"  â€¢ ì„±ê³µë¥ : {(self.success_count/self.processed_count*100):.2f}%")
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"ğŸš€ í‰ê·  ì²˜ë¦¬ ì†ë„: {self.processed_count/total_time:.2f} ì œí’ˆ/ì´ˆ")
        
        return True


async def main():
    """Main function to run bulk synchronization"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Bulk synchronization with checkpoints')
    parser.add_argument('--batch-size', type=int, default=100, help='Batch size for processing')
    parser.add_argument('--no-resume', action='store_true', help='Start fresh without resuming')
    parser.add_argument('--dry-run', action='store_true', help='Show what would be processed without actually processing')
    
    args = parser.parse_args()
    
    if args.dry_run:
        print("ğŸ” ë“œë¼ì´ ëŸ° ëª¨ë“œ: ì‹¤ì œ ì²˜ë¦¬ ì—†ì´ ê³„íšë§Œ í™•ì¸")
        
        # Quick count check
        config = get_settings()
        pg_manager = PostgreSQLManager()
        
        async with pg_manager.get_connection() as conn:
            result = await conn.fetchrow("""
                SELECT COUNT(*) as total 
                FROM product 
                WHERE is_conversion = false AND status = 1
            """)
            total_products = result['total']
        
        total_batches = (total_products + args.batch_size - 1) // args.batch_size
        
        print(f"ğŸ“Š ì´ ì²˜ë¦¬ ëŒ€ìƒ: {total_products:,}ê°œ ì œí’ˆ")
        print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
        print(f"ğŸ“¦ ì´ ë°°ì¹˜ ìˆ˜: {total_batches}")
        print(f"â±ï¸ ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: {total_products * 0.5:.1f}ì´ˆ (ì œí’ˆë‹¹ 0.5ì´ˆ ê°€ì •)")
        return
    
    # Run the bulk synchronization
    synchronizer = BulkSynchronizer(batch_size=args.batch_size)
    resume = not args.no_resume
    
    success = await synchronizer.run_bulk_sync(resume=resume)
    
    if success:
        print("âœ… ëŒ€ëŸ‰ ë™ê¸°í™”ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ëŒ€ëŸ‰ ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main()) 