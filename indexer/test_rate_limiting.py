#!/usr/bin/env python3
# test_rate_limiting.py - Rate Limiting ë° Exponential Backoff í…ŒìŠ¤íŠ¸

import time
from unittest.mock import patch, MagicMock
import openai
from src.services.embedding_service import RateLimiter, EmbeddingService, EmbeddingConfig

def test_rate_limiter():
    print("=== Rate Limiter ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
    
    # í…ŒìŠ¤íŠ¸ìš© ë‚®ì€ ì œí•œìœ¼ë¡œ ì„¤ì •
    rate_limiter = RateLimiter(
        requests_per_minute=10,    # ë¶„ë‹¹ 10ê°œ ìš”ì²­
        tokens_per_minute=5000     # ë¶„ë‹¹ 5000 í† í°
    )
    
    print(f"Rate Limiter ì„¤ì •:")
    print(f"  ë¶„ë‹¹ ìš”ì²­ ì œí•œ: 10")
    print(f"  ë¶„ë‹¹ í† í° ì œí•œ: 5000")
    
    # 1. ì •ìƒ ìš”ì²­ í…ŒìŠ¤íŠ¸
    print(f"\n1. ì •ìƒ ìš”ì²­ í…ŒìŠ¤íŠ¸")
    can_request = rate_limiter.can_make_request(100)
    print(f"   100 í† í° ìš”ì²­ ê°€ëŠ¥: {can_request}")
    
    if can_request:
        rate_limiter.record_request(100)
        print(f"   100 í† í° ìš”ì²­ ê¸°ë¡ë¨")
    
    # 2. ëŒ€ëŸ‰ í† í° ìš”ì²­ í…ŒìŠ¤íŠ¸
    print(f"\n2. ëŒ€ëŸ‰ í† í° ìš”ì²­ í…ŒìŠ¤íŠ¸")
    large_tokens = 6000  # ì œí•œì„ ì´ˆê³¼í•˜ëŠ” í† í°
    can_request_large = rate_limiter.can_make_request(large_tokens)
    print(f"   {large_tokens} í† í° ìš”ì²­ ê°€ëŠ¥: {can_request_large}")
    
    wait_time = rate_limiter.wait_time_needed(large_tokens)
    print(f"   í•„ìš”í•œ ëŒ€ê¸° ì‹œê°„: {wait_time:.2f}ì´ˆ")

def test_exponential_backoff():
    print("\n\n=== Exponential Backoff í…ŒìŠ¤íŠ¸ ===")
    
    config = EmbeddingConfig(
        base_delay=0.1,    # í…ŒìŠ¤íŠ¸ìš© ì§§ì€ ë”œë ˆì´
        max_delay=2.0,     # í…ŒìŠ¤íŠ¸ìš© ì§§ì€ ìµœëŒ€ ë”œë ˆì´
        max_retries=5
    )
    
    print(f"Exponential Backoff ì„¤ì •:")
    print(f"  ê¸°ë³¸ ë”œë ˆì´: {config.base_delay}ì´ˆ")
    print(f"  ìµœëŒ€ ë”œë ˆì´: {config.max_delay}ì´ˆ")
    print(f"  ìµœëŒ€ ì¬ì‹œë„: {config.max_retries}ë²ˆ")
    
    # Mock ì„œë¹„ìŠ¤ ìƒì„±
    with patch('src.services.embedding_service.OpenAI') as mock_openai:
        with patch('src.services.embedding_service.AsyncOpenAI') as mock_async_openai:
            mock_client = MagicMock()
            mock_openai.return_value = mock_client
            mock_async_openai.return_value = mock_client
            
            service = EmbeddingService(api_key="sk-test-dummy", config=config)
            
            print(f"\nì‹œë„ë³„ ë°±ì˜¤í”„ ë”œë ˆì´:")
            for attempt in range(config.max_retries):
                delay = service._exponential_backoff(attempt)
                print(f"  ì‹œë„ {attempt + 1}: {delay:.3f}ì´ˆ")

def test_error_handling():
    print("\n\n=== ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ===")
    
    config = EmbeddingConfig(
        base_delay=0.1,
        max_delay=1.0,
        max_retries=3
    )
    
    with patch('src.services.embedding_service.OpenAI') as mock_openai:
        with patch('src.services.embedding_service.AsyncOpenAI') as mock_async_openai:
            mock_client = MagicMock()
            mock_openai.return_value = mock_client
            mock_async_openai.return_value = mock_client
            
            service = EmbeddingService(api_key="sk-test-dummy", config=config)
            
            # Rate Limit ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            print("1. Rate Limit ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
            mock_response = MagicMock()
            rate_limit_error = openai.RateLimitError(
                message="Rate limit exceeded",
                response=mock_response,
                body={}
            )
            
            should_retry = service._handle_api_error(rate_limit_error, 0)
            print(f"   Rate Limit ì—ëŸ¬ ì¬ì‹œë„ ì—¬ë¶€: {should_retry}")

def main():
    print("ğŸš€ Rate Limiting ë° Exponential Backoff í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    
    test_rate_limiter()
    test_exponential_backoff()
    test_error_handling()
    
    print("\nğŸ‰ ëª¨ë“  Rate Limiting í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 