import os
import time
import logging
from datetime import datetime

from fastapi import APIRouter, HTTPException, status, Depends, Request
from fastapi.responses import JSONResponse
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import Document

from src.config import get_settings
from src.auth.jwt_utils import create_access_token, verify_access_token
from src.api.schema import *
from src.api.document_utils import format_docs, load_txt_documents
from src.auth.user_service import get_current_user

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("api")
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

config = get_settings()
api_router = APIRouter()

# ë²¡í„°ìŠ¤í† ì–´ ë° ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ)
logger.info("ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘...")
DATA_DIR = os.path.join(os.path.dirname(__file__), "../../data")
docs_list = load_txt_documents(DATA_DIR)
vectorstore = Chroma.from_documents(
    documents=docs_list,
    collection_name="rag-chroma",
    embedding=OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY, model="text-embedding-3-large"),
)
retriever = vectorstore.as_retriever()
logger.info(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ. ë¬¸ì„œ {len(docs_list)}ê°œ ë¡œë“œë¨")

@api_router.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest, user=Depends(get_current_user)):
    request_id = f"req_{int(time.time())}"
    start_time = time.time()
    logger.info(f"ğŸ” [{request_id}] ê²€ìƒ‰ ìš”ì²­ ì‹œì‘ - ì‚¬ìš©ì: {user}, ì¿¼ë¦¬: '{request.question}'")
    
    try:
        input_query = request.question
        system_instructions = """
        ë„ˆëŠ” ë°”ì´í¬ ì¤‘ê³ ë§¤ë¬¼ì„ ì‚¬ìš©ìì—ê²Œ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ë‚´ê°€ ì œê³µí•˜ëŠ” ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸ ì¤‘ì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë§¤ë¬¼ë“¤ì„ ì„ ë³„í•˜ì—¬ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤.
        
        ë°˜ë“œì‹œ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•˜ë©°, ê° ë§¤ë¬¼ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì‘ì„±í•˜ì„¸ìš”:
        - rank: ì¶”ì²œ ìˆœìœ„ (1ì´ ê°€ì¥ ë†’ì€ ìˆœìœ„, 2, 3, ... ìˆœìœ¼ë¡œ ë‚®ì•„ì§)
        - title: ë§¤ë¬¼ì˜ ì œëª© (ì˜ˆ: ì•¼ë§ˆí•˜ R3 2018)
        - url: ë§¤ë¬¼ ìƒì„¸ í˜ì´ì§€ URL (ì˜ˆ: https://example.com/listings/ID)
        - img_url: ë§¤ë¬¼ ì´ë¯¸ì§€ URL (ì˜ˆ: https://example.com/images/ID.jpg)
        - price: ë§¤ë¬¼ ê°€ê²© (ìˆ«ìë¡œë§Œ í‘œê¸°, ì˜ˆ: 4600000)
        - content: ë§¤ë¬¼ì˜ ë‚´ìš©. ì´ë¯¸ ìš”ì•½ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ê·¸ëŒ€ë¡œ ì œê³µ
        - match_summary: ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë§¤ë¬¼ì˜ ì—°ê´€ì„± ì„¤ëª… (ì¶”ì²œí•˜ëŠ” ì´ìœ )
        
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ëŠ” ë§¤ë¬¼ì„ ì„ ë³„í•˜ì—¬ ì¶”ì²œí•˜ê³ , ë°˜ë“œì‹œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì— ë”°ë¼ ìˆœìœ„ë¥¼ ë§¤ê²¨ì„œ ì œê³µí•˜ì„¸ìš”.
        ìˆœìœ„ 1ì´ ê°€ì¥ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ë§¤ë¬¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        """
        
        # Prompt í…œí”Œë¦¿ ì •ì˜
        prompt_template = f"""
            # ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­
            {system_instructions}
            
            # ì‚¬ìš©ìê°€ ì°¾ëŠ” ë§¤ë¬¼ ìƒì„¸ ë‚´ìš©
            {input_query}

            # ì¤‘ê³ ë§¤ë¬¼ì„ ì¶”ì²œí•˜ê¸° ìœ„í•´ ì„ ë³„í•œ ìœ ì‚¬í•œ ë§¤ë¬¼(ë‚´ìš©)
            {{context}}
        """
        prompt = PromptTemplate.from_template(prompt_template)
        
        logger.info(f"ğŸ“„ [{request_id}] ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
        
        # with_structured_outputìœ¼ë¡œ PropertyItems í˜•ì‹ ì§€ì •
        llm_model = ChatOpenAI(
            model_name="gpt-4o",
            api_key=config.OPENAI_API_KEY,
            temperature=0.1  # ì¼ê´€ëœ ì¶œë ¥ì„ ìœ„í•´ ë‚®ì€ temperature ì„¤ì •
        ).with_structured_output(PropertyItems)

        chain = (
            {
                "context": retriever | format_docs
            }
            | prompt
            | llm_model
        )
        
        vector_search_query = input_query
        logger.info(f"ğŸ¤– [{request_id}] LLMì— ì¶”ì²œ ìš”ì²­ ì¤‘...")
        property_items = await chain.ainvoke(vector_search_query)
        
        # PropertyItemsì—ì„œ items í•„ë“œë¥¼ ì¶”ì¶œí•˜ì—¬ QueryResponseì— ë„£ìŒ
        # ì‘ë‹µ ê²°ê³¼ë¥¼ ìˆœìœ„(rank)ì— ë”°ë¼ ì •ë ¬
        sorted_items = sorted(property_items.items, key=lambda item: item.rank)
        
        response = QueryResponse(
            result=sorted_items,
            message="ì„±ê³µì ìœ¼ë¡œ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤",
            success=True
        )
        
        process_time = time.time() - start_time
        logger.info(f"âœ… [{request_id}] ê²€ìƒ‰ ì™„ë£Œ: {len(sorted_items)}ê°œ ë§¤ë¬¼ ì¶”ì²œ, ì²˜ë¦¬ ì‹œê°„: {process_time:.2f}ì´ˆ")
        
        return response
        
    except Exception as e:
        process_time = time.time() - start_time
        if isinstance(e, ValueError):
            logger.error(f"âŒ [{request_id}] ì˜ëª»ëœ ìš”ì²­: {str(e)}, ì²˜ë¦¬ ì‹œê°„: {process_time:.2f}ì´ˆ")
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
        elif isinstance(e, KeyError):
            logger.error(f"âŒ [{request_id}] ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}, ì²˜ë¦¬ ì‹œê°„: {process_time:.2f}ì´ˆ")
            raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=str(e))
        else:
            logger.error(f"âŒ [{request_id}] ì„œë²„ ì˜¤ë¥˜: {str(e)}, ì²˜ë¦¬ ì‹œê°„: {process_time:.2f}ì´ˆ", exc_info=True)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
                detail="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: " + str(e)
            )
